# InternVL-Chat

This folder contains the implementation of the InternVL-Chat.

## üõ†Ô∏è Installation

> If you have already installed the environment as per the instructions in other folders, you can skip this section.

- Clone this repository:

  ```bash
  git clone https://github.com/OpenGVLab/InternVL.git
  cd InternVL/internvl_chat
  ```

- Create a conda virtual environment and activate it:

  ```bash
  conda create -n internvl python=3.9 -y
  conda activate internvl
  ```

- Install `PyTorch>=2.0` and `torchvision>=0.15.2` with `CUDA>=11.6`:

  For examples, to install `torch==2.0.1` with `CUDA==11.8`:

  ```bash
  conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia
  # or
  pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
  ```

- Install `flash-attn==0.2.8` :

  If you want to fully replicate my results, please install `v0.2.8`, otherwise install the latest version.

  This is because different versions of flash attention yield slight differences in results.

  ```bash
  git clone https://github.com/Dao-AILab/flash-attention.git
  cd flash-attention
  git checkout v0.2.8
  python setup.py install
  ```

- Install `timm==0.6.11` and `mmcv-full==1.6.2`:

  ```bash
  pip install -U openmim
  pip install timm==0.6.11
  mim install mmcv-full==1.6.2
  ```

- Install `apex`:

  ```bash
  git clone https://github.com/NVIDIA/apex.git
  git checkout 2386a912164b0c5cfcd8be7a2b890fbac5607c82  # https://github.com/NVIDIA/apex/issues/1735
  pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings "--build-option=--cpp_ext" --config-settings "--build-option=--cuda_ext" ./
  ```

- Install other requirements:

  ```bash
  pip install opencv-python termcolor yacs pyyaml scipy
  pip install deepspeed==0.10.0
  pip install pycocoevalcap tqdm
  ```

## üì¶ Model Preparation

| model name         | type        | download                                                          |  size   |
| ------------------ | ----------- | ----------------------------------------------------------------- | :-----: |
| InternVL-14B-224px | huggingface | ü§ó [HF link](https://huggingface.co/OpenGVLab/InternVL-14B-224px) | 27.7 GB |
| Vicuna-7B-v1.5     | huggingface | ü§ó [HF link](https://huggingface.co/lmsys/vicuna-7b-v1.5)         | 13.5 GB |
| Vicuna-13B-v1.5    | huggingface | ü§ó [HF link](https://huggingface.co/lmsys/vicuna-13b-v1.5)        | 26.1 GB |

Please download the above model weights and place them in the `pretrained/` folder.

```sh
cd pretrained/
# pip install -U huggingface_hub
huggingface-cli download --resume-download --local-dir-use-symlinks False OpenGVLab/InternVL-14B-224px --local-dir internvl_14b_224px
huggingface-cli download --resume-download --local-dir-use-symlinks False lmsys/vicuna-13b-v1.5 --local-dir vicuna-13b-v1.5
huggingface-cli download --resume-download --local-dir-use-symlinks False lmsys/vicuna-7b-v1.5 --local-dir vicuna-7b-v1.5
```

The directory structure is:

```sh
pretrained
‚îÇ‚îÄ‚îÄ internvl_14b_224px/
‚îÇ‚îÄ‚îÄ vicuna-13b-v1.5/
‚îî‚îÄ‚îÄ vicuna-7b-v1.5/
```

## üî• Supervised Fine-tuning

Coming Soon

## üìä Evaluation (English Models)

| model         | QLLaMA | LLM          | res | COCO  | Flickr | NoCaps | VQAv2 | GQA  | VizWiz | TextVQA | MME    | POPE | Download |
| ------------- | ------ | ------------ | --- | ----- | ------ | ------ | ----- | ---- | ------ | ------- | ------ | ---- | -------- |
| InternVL-Chat | ‚úîÔ∏è     | frozen V-7B  | 224 | 141.4 | 89.7   | 120.5  | 72.3  | 57.7 | 44.5   | 42.1    | 1298.5 | 85.2 | TODO     |
| InternVL-Chat | ‚úîÔ∏è     | frozen V-13B | 224 | 142.4 | 89.9   | 123.1  | 71.7  | 59.5 | 54.0   | 49.1    | 1317.2 | 85.4 | TODO     |
| InternVL-Chat | ‚úîÔ∏è     | V-13B        | 336 | 146.2 | 92.2   | 126.2  | 81.2  | 66.6 | 58.5   | 61.5    | 1586.4 | 87.6 | TODO     |

## üìä Evaluation (Chinese Models)

**MultiModal Benchmark**

| model                                                                             | MME            | MMB<sub>dev/test</sub> | MMB-CN<sub>dev/test</sub> | POPE | MMMU | CMMMU | Tiny LVLM | LLaVA<sub>bench</sub> |
| --------------------------------------------------------------------------------- | -------------- | ---------------------- | ------------------------- | ---- | ---- | ----- | --------- | --------------------- |
| [InternVL-Chat-V1.1](https://huggingface.co/OpenGVLab/InternVL-Chat-V1.1-Chinese) | 1672.4 / 341.1 | 76.6 / 75.4            | 71.5 / 70.1               | 87.2 |      |       | 344.5     | 76.3                  |

**Visual Question Answering**

| model                                                                             | VQAv2<sub>test</sub> | OKVQA<sub>val</sub> | TextVQA<sub>val</sub> | VizWiz<sub>val/test</sub> | AI2D<sub>test</sub> | GQA<sub>test</sub> | SQA<sub>test</sub> |
| --------------------------------------------------------------------------------- | -------------------- | ------------------- | --------------------- | ------------------------- | ------------------- | ------------------ | ------------------ |
| [InternVL-Chat-V1.1](https://huggingface.co/OpenGVLab/InternVL-Chat-V1.1-Chinese) | 80.9                 | 64.2                | 65.8                  | 58.3 / 57.3               | 70.23               | 62.4               | 91.2               |

**Image Captioning**

| model                                                                             | COCO<sub>test</sub> | Flickr30K<sub>test</sub> | NoCaps<sub>val</sub> |
| --------------------------------------------------------------------------------- | ------------------- | ------------------------ | -------------------- |
| [InternVL-Chat-V1.1](https://huggingface.co/OpenGVLab/InternVL-Chat-V1.1-Chinese) | 141.8               | 84.3                     | 120.4                |

## ‚ùì How to Evaluate

Please prepare the data according to the following directory structure.

<details>
<summary>Directory Structure</summary>

```
data
‚îú‚îÄ‚îÄ flickr30k
‚îÇ   ‚îú‚îÄ‚îÄ flickr30k_test_karpathy.json
‚îÇ   ‚îî‚îÄ‚îÄ Images/
‚îú‚îÄ‚îÄ coco
‚îÇ   ‚îú‚îÄ‚îÄ annotations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ coco_karpathy_test_gt.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ coco_karpathy_test.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ train2014/
‚îÇ   ‚îú‚îÄ‚îÄ val2014/
‚îÇ   ‚îî‚îÄ‚îÄ test2015/
‚îú‚îÄ‚îÄ nocaps
‚îÇ   ‚îú‚îÄ‚îÄ nocaps_val_4500_captions.json
‚îÇ   ‚îî‚îÄ‚îÄ images/
‚îú‚îÄ‚îÄ vqav2
‚îÇ   ‚îú‚îÄ‚îÄ v2_mscoco_train2014_annotations.json
‚îÇ   ‚îú‚îÄ‚îÄ v2_mscoco_train2014_complementary_pairs.json
‚îÇ   ‚îú‚îÄ‚îÄ v2_mscoco_val2014_annotations.json
‚îÇ   ‚îú‚îÄ‚îÄ v2_OpenEnded_mscoco_test2015_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ v2_OpenEnded_mscoco_test-dev2015_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ v2_OpenEnded_mscoco_train2014_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ v2_OpenEnded_mscoco_val2014_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ vqav2_testdev.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ vqav2_train.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ vqav2_val.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ train2014/ -> ../coco/train2014/
‚îÇ   ‚îú‚îÄ‚îÄ val2014/ -> ../coco/val2014/
‚îÇ   ‚îî‚îÄ‚îÄ test2015/ -> ../coco/test2015/
‚îú‚îÄ‚îÄ okvqa
‚îÇ   ‚îú‚îÄ‚îÄ mscoco_train2014_annotations.json
‚îÇ   ‚îú‚îÄ‚îÄ mscoco_val2014_annotations.json
‚îÇ   ‚îú‚îÄ‚îÄ OpenEnded_mscoco_train2014_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ OpenEnded_mscoco_val2014_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ okvqa_train.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ okvqa_val.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ train2014/ -> ../coco/train2014/
‚îÇ   ‚îî‚îÄ‚îÄ val2014/ -> ../coco/val2014/
‚îú‚îÄ‚îÄ textvqa
‚îÇ   ‚îú‚îÄ‚îÄ textvqa_train_annotations.json
‚îÇ   ‚îú‚îÄ‚îÄ textvqa_train.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ textvqa_train_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ textvqa_val_annotations.json
‚îÇ   ‚îú‚îÄ‚îÄ textvqa_val.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ textvqa_val_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ textvqa_val_llava.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ train_images/
‚îú‚îÄ‚îÄ vizwiz
‚îÇ   ‚îú‚îÄ‚îÄ vizwiz_test.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ vizwiz_train_annotations.json
‚îÇ   ‚îú‚îÄ‚îÄ vizwiz_train.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ vizwiz_train_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ vizwiz_val_annotations.json
‚îÇ   ‚îú‚îÄ‚îÄ vizwiz_val.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ vizwiz_val_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îî‚îÄ‚îÄ annotations/
‚îú‚îÄ‚îÄ docvqa
‚îÇ   ‚îú‚îÄ‚îÄ test.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ val.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îî‚îÄ‚îÄ val/
‚îú‚îÄ‚îÄ chartqa
‚îÇ   ‚îú‚îÄ‚îÄ ChartQA Dataset/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val/
‚îÇ   ‚îú‚îÄ‚îÄ test_augmented.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ test_human.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ train_augmented.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ train_human.jsonl
‚îú‚îÄ‚îÄ gqa
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ eval.py
‚îÇ   ‚îú‚îÄ‚îÄ challenge_all_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ challenge_balanced_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ llava_gqa_testdev_balanced_qwen_format.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ submission_all_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ test_all_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ test_balanced.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ test_balanced_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ testdev_all_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ testdev_balanced_all_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ testdev_balanced_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ train_all_questions/
‚îÇ   ‚îú‚îÄ‚îÄ train_balanced.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ train_balanced_questions.json
‚îÇ   ‚îú‚îÄ‚îÄ val_all_questions.json
‚îÇ   ‚îî‚îÄ‚îÄ val_balanced_questions.json
‚îú‚îÄ‚îÄ ocrvqa
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ ocrvqa_test.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ ocrvqa_train.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ ocrvqa_val.jsonl
‚îú‚îÄ‚îÄ ai2diagram
‚îÇ   ‚îú‚îÄ‚îÄ ai2d/
‚îÇ   ‚îú‚îÄ‚îÄ test.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ train.jsonl
‚îú‚îÄ‚îÄ scienceqa
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ problems.json
‚îÇ   ‚îî‚îÄ‚îÄ scienceqa_test_img.jsonl
‚îú‚îÄ‚îÄ refcoco
‚îÇ   ‚îú‚îÄ‚îÄ refcocog_test.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ refcocog_val.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ refcoco_testA.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ refcoco+_testA.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ refcoco_testB.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ refcoco+_testB.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ refcoco_val.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ refcoco+_val.jsonl
‚îú‚îÄ‚îÄ mme
‚îÇ   ‚îú‚îÄ‚îÄ MME_Benchmark_release/
‚îÇ   ‚îî‚îÄ‚îÄ images/
‚îú‚îÄ‚îÄ pope
‚îÇ   ‚îú‚îÄ‚îÄ coco/
‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ coco_pope_adversarial.json
‚îÇ   ‚îÇ    ‚îú‚îÄ‚îÄ coco_pope_popular.json
‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ coco_pope_random.json
‚îÇ   ‚îú‚îÄ‚îÄ val2014/ -> ../coco/val2014/
‚îÇ   ‚îî‚îÄ‚îÄ llava_pope_test.jsonl
‚îú‚îÄ‚îÄ tiny_lvlm
‚îÇ   ‚îî‚îÄ‚îÄ updated_datasets
‚îÇ       ‚îú‚îÄ‚îÄ Object_Hallucination
‚îÇ       ‚îú‚îÄ‚îÄ ...
‚îÇ       ‚îî‚îÄ‚îÄ Visual_Reasoning
‚îú‚îÄ‚îÄ mmbench
‚îÇ   ‚îú‚îÄ‚îÄ mmbench_dev_20230712.tsv
‚îÇ   ‚îú‚îÄ‚îÄ mmbench_dev_cn_20231003.tsv
‚îÇ   ‚îú‚îÄ‚îÄ mmbench_dev_en_20231003.tsv
‚îÇ   ‚îú‚îÄ‚îÄ mmbench_test_cn_20231003.tsv
‚îÇ   ‚îî‚îÄ‚îÄ mmbench_test_en_20231003.tsv
‚îú‚îÄ‚îÄ llava-bench-in-the-wild
‚îÇ   ‚îú‚îÄ‚îÄ answers_gpt4.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ images/
‚îú‚îÄ‚îÄ mmmu
‚îÇ   ‚îú‚îÄ‚îÄ Accounting/
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ Sociology
```

</details>

### Image Caption

#### [COCO karpathy test](https://cocodataset.org/)

> COCO images are used in VQAv2/OK-VQA/RefCOCO/RefCOCO+/RefCOCOg. Make sure you have already downloaded COCO images before evaluating on these benchmarks.

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/coco && cd data/coco

# download coco images
wget http://images.cocodataset.org/zips/train2014.zip && unzip train2014.zip
wget http://images.cocodataset.org/zips/val2014.zip && unzip val2014.zip
wget http://images.cocodataset.org/zips/test2015.zip && unzip test2015.zip

mkdir -p annotations && cd annotations/
# download converted annotation files
wget https://github.com/OpenGVLab/InternVL/releases/download/data/coco_karpathy_test.json
wget https://github.com/OpenGVLab/InternVL/releases/download/data/coco_karpathy_test_gt.json

cd ../../../
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> caption-coco
```

</details>

#### [Flickr30K karpathy test](https://bryanplummer.com/Flickr30kEntities/)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/flickr30k && cd data/flickr30k

# download images from https://bryanplummer.com/Flickr30kEntities/
# karpathy split annotations can be downloaded from https://cs.stanford.edu/people/karpathy/deepimagesent/
# download converted files
wget https://github.com/OpenGVLab/InternVL/releases/download/data/flickr30k_test_karpathy.json

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> caption-flickr30k
```

</details>

#### [NoCaps val](https://nocaps.org/)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/nocaps && cd data/nocaps

# download images from https://nocaps.org/download
# original annotations can be downloaded from https://nocaps.s3.amazonaws.com/nocaps_val_4500_captions.json
wget https://nocaps.s3.amazonaws.com/nocaps_val_4500_captions.json

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> caption-nocaps
```

</details>

### General VQA

#### [VQAv2 val & test-dev](https://visualqa.org/)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/vqav2 && cd data/vqav2

# make sure you have downloaded COCO images
ln -s ../coco/train2014 ./
ln -s ../coco/val2014 ./
ln -s ../coco/test2015 ./

# download questions and annotations
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip && unzip v2_Annotations_Train_mscoco.zip
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip && unzip v2_Questions_Train_mscoco.zip
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip && unzip v2_Annotations_Val_mscoco.zip
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip && unzip v2_Questions_Val_mscoco.zip
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip && unzip v2_Questions_Test_mscoco.zip

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vqav2/vqav2_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vqav2/vqav2_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vqav2/vqav2_testdev.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
# VQAv2-val
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-vqav2-val
# VQAv2-testdev
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-vqav2-testdev
```

For the testdev set, submit the results to the [evaluation server](https://eval.ai/web/challenges/challenge-page/830/my-submission).

</details>

#### [OKVQA val](https://okvqa.allenai.org/)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/okvqa && cd data/okvqa

# make sure you have downloaded COCO images
ln -s ../coco/train2014 ./
ln -s ../coco/val2014 ./

# download annotations and questions
wget https://okvqa.allenai.org/static/data/mscoco_train2014_annotations.json.zip && unzip mscoco_train2014_annotations.json.zip
wget https://okvqa.allenai.org/static/data/OpenEnded_mscoco_train2014_questions.json.zip && unzip OpenEnded_mscoco_train2014_questions.json.zip
wget https://okvqa.allenai.org/static/data/mscoco_val2014_annotations.json.zip && unzip mscoco_val2014_annotations.json.zip
wget https://okvqa.allenai.org/static/data/OpenEnded_mscoco_val2014_questions.json.zip && unzip OpenEnded_mscoco_val2014_questions.json.zip

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/okvqa/okvqa_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/okvqa/okvqa_val.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-okvqa-val
```

</details>

#### [TextVQA val](https://textvqa.org/)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/textvqa && cd data/textvqa

# download images
wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip && unzip train_val_images.zip

# download annotations and questions
wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_train.json
wget https://dl.fbaipublicfiles.com/textvqa/data/TextVQA_0.5.1_val.json

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_train_annotations.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_train_questions.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_val_annotations.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_val_questions.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_val.jsonl
wget https://github.com/OpenGVLab/InternVL/releases/download/data/textvqa_val_llava.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-textvqa-val
```

</details>

#### [VizWiz val & test](https://vizwiz.org/tasks-and-datasets/vqa/)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/vizwiz && cd data/vizwiz

# download images
wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/train.zip && unzip train.zip
wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/val.zip && unzip val.zip
wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/test.zip && unzip test.zip

# download annotations
wget https://vizwiz.cs.colorado.edu/VizWiz_final/vqa_data/Annotations.zip && unzip Annotations.zip

# download converted files
# train
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_train_annotations.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_train_questions.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_train.jsonl
# val
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_val_annotations.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_val_questions.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_val.jsonl
# test
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_test.jsonl
cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
# VizWiz val
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-vizwiz-val
# VizWiz test
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-vizwiz-test
```

For the test set, submit the results to the [evaluation server](https://eval.ai/web/challenges/challenge-page/1911/my-submission).

</details>

#### [DocVQA val & test](https://www.docvqa.org/datasets)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/docvqa && cd data/docvqa

# download images and annotations from https://www.docvqa.org/datasets

# download converted files
# train
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/docvqa/train.jsonl
# val
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/docvqa/val.jsonl
# test
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/docvqa/test.jsonl
cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
# DocVQA-val
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-docvqa-val
# DocVQA-test
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-docvqa-test
```

For the test set, submit the results to the [evaluation server](https://rrc.cvc.uab.es/?ch=17).

</details>

#### [ChartQA test-human & test-augmented](https://aclanthology.org/2022.findings-acl.177/)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/chartqa && cd data/chartqa

# download images from https://drive.google.com/file/d/1Lm_w6zeET1Hyl_9ks6w5nEsgpoyPHalV/view

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/chartqa/train_human.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/chartqa/train_augmented.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/chartqa/test_human.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/chartqa/test_augmented.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
# ChartQA-test-human
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-chartqa-test-human
# ChartQA-test-augmented
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-chartqa-test-augmented
```

</details>

#### [GQA testdev](https://cs.stanford.edu/people/dorarad/gqa/about.html)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/gqa && cd data/gqa

# download images
wget https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip
unzip images.zip

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/gqa/testdev_balanced.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/gqa/train_balanced.jsonl
wget https://github.com/OpenGVLab/InternVL/releases/download/data/llava_gqa_testdev_balanced_qwen_format.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-gqa-testdev
```

</details>

#### [OCRVQA val & test](https://ocr-vqa.github.io/)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/ocrvqa && cd data/ocrvqa

# download images by following instructions at https://ocr-vqa.github.io/kvqa_ProjectFiles/README.txt

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/ocrvqa/ocrvqa_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/ocrvqa/ocrvqa_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/ocrvqa/ocrvqa_test.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
# OCRVQA-val
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-ocrvqa-val
# OCRVQA-test
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-ocrvqa-test
```

</details>

#### [AI2Diagram test](https://allenai.org/data/diagrams)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/ai2diagram && cd data/ai2diagram

# download images
wget https://ai2-public-datasets.s3.amazonaws.com/diagrams/ai2d-all.zip

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/ai2diagram/train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/ai2diagram/test.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> vqa-ai2d-test
```

</details>

#### [ScienceQA test](https://github.com/lupantech/ScienceQA)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/scienceqa/images && cd data/scienceqa/images

# download images
wget https://scienceqa.s3.us-west-1.amazonaws.com/images/test.zip && unzip test.zip

cd ..

# download original questions
wget https://github.com/lupantech/ScienceQA/blob/main/data/scienceqa/problems.json

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/scienceqa/scienceqa_test_img.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> scienceqa
```

</details>

### Refer Expression Comprehension

#### RefCOCO/RefCOCO+/RefCOCO-g

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/refcoco && cd data/refcoco

# download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco/refcoco_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco/refcoco_testA.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco/refcoco_testB.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco%2B/refcoco%2B_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco%2B/refcoco%2B_testA.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco%2B/refcoco%2B_testB.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcocog/refcocog_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcocog/refcocog_test.jsonl

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> refcoco
```

</details>

### MultiModal Dialogue

#### [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/mme && cd data/mme

# 1. Download MME images and eval_tool from the [MME repo](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/blob/Evaluation/README.md)
# 2. Rearrange images by executing `python get_images.py`
python get_images.py
cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
# single GPU testing
CUDA_VISIBLE_DEVICES=0 sh evaluate.sh <checkpoint> mme
```

</details>

#### [MMBench dev & test](https://github.com/open-compass/mmbench/?tab=readme-ov-file)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/mmbench && cd data/mmbench

# download csv files of mmbench
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_dev_20230712.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_dev_cn_20231003.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_dev_en_20231003.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_test_cn_20231003.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_test_en_20231003.tsv

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
# mmbench_dev_20230712
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> mmbench-dev-en
# mmbench_dev_cn_20231003
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> mmbench-dev-cn
# mmbench_test_en_20231003
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> mmbench-test-en
# mmbench_test_cn_20231003
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> mmbench-test-cn
```

Then, submit the results to the [evaluation server](https://mmbench.opencompass.org.cn/mmbench-submission).

</details>

#### [POPE](https://github.com/AoiDragon/POPE/tree/main)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/pope && cd data/pope

# make sure you have downloaded COCO images
ln -s ../coco/val2014 ./
wget https://github.com/OpenGVLab/InternVL/releases/download/data/llava_pope_test.jsonl

# download `coco` from POPE
mkdir -p coco && cd coco
wget https://github.com/AoiDragon/POPE/raw/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco/coco_pope_adversarial.json
wget https://github.com/AoiDragon/POPE/raw/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco/coco_pope_popular.json
wget https://github.com/AoiDragon/POPE/raw/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco/coco_pope_random.json
cd ../../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> pope
```

</details>

#### MMMU

<details>
<summary>Data Preparation</summary>

The evaluation code will automatically download the dataset from hugging face.

</details>

<details>
<summary>Evaluation</summary>

```bash
# dev set
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> mmmu-dev
# val set
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> mmmu-val
# test set
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> mmmu-test
```

Then, submit the results to the [evaluation server](https://eval.ai/web/challenges/challenge-page/2179/overview).

</details>

#### CMMMU

#### [Tiny LVLM](https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation)

<details>
<summary>Data Preparation</summary>

```bash
mkdir -p data/tiny_lvlm && cd data/tiny_lvlm

# download dataset from https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/tiny_lvlm_evaluation
# i.e., download `updated_datasets.tar.gz` from https://drive.google.com/file/d/1PuFC612XzOmKwzRldtBb1CFZnIjiR7we/view
tar -xzvf updated_datasets.tar.gz

cd ../..
```

</details>

<details>
<summary>Evaluation</summary>

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 sh evaluate.sh <checkpoint> tiny_lvlm
```

</details>

#### [LLaVA Bench](https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild)

<details>
<summary>Data Preparation</summary>

```bash
cd data/
# download dataset from https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild
git clone https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild
cd llava-bench-in-the-wild/
rm -rf images && mkdir -p images && cd images
# download all 24 images
wget https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild/resolve/main/images/001.jpg
# ...
wget https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-wild/resolve/main/images/024.jpg
cd ../../../
```

</details>

<details>
<summary>Evaluation</summary>

```bash
# single GPU testing
export OPENAI_API_KEY='your_gpt4_key'
CUDA_VISIBLE_DEVICES=0 sh evaluate.sh <checkpoint> llava-bench
```

</details>
