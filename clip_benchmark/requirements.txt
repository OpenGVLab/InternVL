open_clip_torch>=0.2.1
pycocoevalcap
scikit-learn>=1.0,<2
torch>=1.8.1,<2
torchvision>=0.8.9,<2
tqdm>=2
transformers
webdataset>=0.2.31
