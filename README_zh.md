<div align="center">

# InternVLå®¶æ—ï¼šé€šè¿‡å¼€æºç»„ä»¶ç¼©å°ä¸å•†ä¸šå¤šæ¨¡æ€æ¨¡å‹çš„å·®è· â€”â€” GPT-4oçš„å¼€æºæ›¿ä»£æ–¹æ¡ˆ

<div align="center">
  <img width="500" alt="image" src="https://github.com/user-attachments/assets/930e6814-8a9f-43e1-a284-118a5732daa4">
  <br>
</div>

[\[ğŸ†• åšå®¢\]](https://internvl.github.io/blog/) [\[ğŸ¤” å¸¸è§é—®é¢˜\]](https://internvl.readthedocs.io/en/latest/tutorials/faqs.html)  [\[ğŸ—¨ï¸ å¯¹è¯Demo\]](https://internvl.opengvlab.com/)  [\[ğŸ¤— HF Demo\]](https://huggingface.co/spaces/OpenGVLab/InternVL)  [\[ğŸ“– æ–‡æ¡£\]](https://internvl.readthedocs.io/en/latest/)  [\[ğŸŒ API\]](https://internlm.intern-ai.org.cn/api/document)  [\[ğŸš€ å¿«é€Ÿå¼€å§‹\]](#ä½¿ç”¨-huggingface-å¿«é€Ÿå¼€å§‹)

[\[ğŸ“œ InternVL 2.5 æŠ¥å‘Š\]](https://huggingface.co/papers/2412.05271) [\[ğŸ”¥ Mini-InternVL è®ºæ–‡\]](https://arxiv.org/abs/2410.16261)  [\[ğŸš€ InternVL2 åšå®¢\]](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/)   [\[ğŸ“œ InternVL 1.5 è®ºæ–‡\]](https://huggingface.co/papers/2404.16821)  [\[ğŸ“œ InternVL 1.0 è®ºæ–‡\]](https://huggingface.co/papers/2312.14238)

[\[ğŸ“– 2.0 ä¸­æ–‡è§£è¯»\]](https://zhuanlan.zhihu.com/p/706547971)  [\[ğŸ“– 1.5 ä¸­æ–‡è§£è¯»\]](https://zhuanlan.zhihu.com/p/699439759)  [\[ğŸ“– 1.0 ä¸­æ–‡è§£è¯»\]](https://zhuanlan.zhihu.com/p/702946079)

[Switch to the English version (åˆ‡æ¢è‡³è‹±æ–‡ç‰ˆ)](/README.md)

<a href="https://trendshift.io/repositories/9803" target="_blank"><img src="https://trendshift.io/api/badge/repositories/9803" alt="OpenGVLab%2FInternVL | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
<img height="55" alt="image" src="https://github.com/user-attachments/assets/bd62ab46-f0ea-40c6-ab10-7fde671716cc">

![image/png](https://huggingface.co/datasets/Weiyun1025/InternVL-Performance/resolve/main/internvl3/overall.png)

</div>

## æœ€æ–°æ¶ˆæ¯ ğŸš€ğŸš€ğŸš€

- `2025/04/17`: æˆ‘ä»¬å¼€æºäº† [MPO](https://huggingface.co/papers/2411.10442) å’Œ [VisualPRM](https://huggingface.co/papers/2503.10291) çš„[æ•°æ®æ„é€ ç®¡çº¿](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/tools/reasoning_data_pipeline)åŠ[è®­ç»ƒè„šæœ¬](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/mpo)ã€‚ æ­¤å¤– [MPO](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/mpo_data_construction) å’Œ [VisualPRM](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/visualprm_data_construction) çš„æ•°æ®æ„å»ºè„šæœ¬ä¹Ÿå·²ç»å¼€æºã€‚
- `2025/04/11`: ğŸš€ æˆ‘ä»¬å‘å¸ƒäº† [InternVL3](https://huggingface.co/collections/OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d)ï¼Œ ä¸€ä¸ªæ€§èƒ½å¼ºå¤§çš„å¼€æºå¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚ å…¶ä¸­ InternVL3-78B åŒæ—¶åœ¨[æ„ŸçŸ¥èƒ½åŠ›](https://rank.opencompass.org.cn/leaderboard-multimodal/?m=REALTIME)å’Œ[æ¨ç†èƒ½åŠ›](https://rank.opencompass.org.cn/leaderboard-multimodal-reasoning/?m=REALTIME)ä¸ŠåŒæ—¶è¾¾åˆ°äº†å¼€æºç¬¬ä¸€çš„æ€§èƒ½ã€‚ InternVL3-78B çš„æ ¸å¿ƒæŠ€æœ¯åŒ…æ‹¬ï¼š[Variable Visual Position Encoding](https://huggingface.co/papers/2412.09616)ï¼Œ[Native Multimodal Pre-Training](https://huggingface.co/papers/2504.10479)ï¼Œ[Mixed Preference Optimization](https://huggingface.co/papers/2411.10442)ï¼Œä»¥åŠ [Multimodal Test-Time Scaling](https://huggingface.co/papers/2503.10291)ã€‚
- `2025/03/13`: ğŸ”¥ æˆ‘ä»¬å‘å¸ƒäº† [VisualPRM](https://huggingface.co/OpenGVLab/VisualPRM-8B)ï¼Œä¸€ä¸ª8Bå‚æ•°ä¸¤çš„å¤šæ¨¡æ€è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰ã€‚è¯¥æ¨¡å‹åœ¨ Best-of-8 çš„è¯„æµ‹è®¾ç½®ä¸‹ä½¿å¾— InternVL2.5-8B å’Œ InternVL2.5-78B åœ¨ä¸ƒä¸ªå¤šæ¨¡æ€æ¨ç†è¯„æµ‹åŸºå‡†ä¸Šçš„ç»¼åˆæ€§èƒ½åˆ†åˆ«æå‡äº† 8.4 å’Œ 5.9 åˆ†ã€‚è¯¥æ¨¡å‹çš„è®­ç»ƒæ•°æ® [VisualPRM400K](https://huggingface.co/datasets/OpenGVLab/VisualPRM400K)ä¹Ÿå·²ç»å¼€æºã€‚è¯·å‚è€ƒæˆ‘ä»¬çš„[è®ºæ–‡](https://huggingface.co/papers/2503.10291)å’Œ[é¡¹ç›®ä¸»é¡µ](https://internvl.github.io/blog/2025-03-13-VisualPRM/)æ¥äº†è§£æ›´å¤šç»†èŠ‚ã€‚
- `2024/12/20`: ğŸ”¥ æˆ‘ä»¬å‘å¸ƒäº† [InternVL2.5-MPOç³»åˆ—](https://internvl.github.io/blog/2024-12-20-InternVL-2.5-MPO/)ã€‚è¯¥ç³»åˆ—é€šè¿‡ [Mixed Preference Optimization](https://huggingface.co/papers/2411.10442) ç®—æ³•å’Œ [MMPR-v1.1](https://huggingface.co/datasets/OpenGVLab/MMPR-v1.1) æ•°æ®é›†å¾®è°ƒå¾—åˆ°ã€‚**è¯¥ç³»åˆ—æ¨¡å‹åœ¨OpenCompassè¯„æµ‹æ¦œå•ä¸­çš„æ•´ä½“æ€§èƒ½è¶…è¿‡MPOè®­ç»ƒå‰ä¸¤ä¸ªç™¾åˆ†ç‚¹ã€‚** è¿™äº›æ¨¡å‹å¯åœ¨ [HF é“¾æ¥](https://huggingface.co/collections/OpenGVLab/internvl25-mpo-6753fed98cd828219b12f849)ä¸­ä¸‹è½½ã€‚
- `2024/12/17`: ğŸš€ Paddleå›¢é˜Ÿå·²åœ¨[PaddleMIX](https://github.com/PaddlePaddle/PaddleMIX)æ¡†æ¶ä¸­é€‚é…[InternVL2/2.5](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/internvl2)ã€‚
- `2024/12/05`: ğŸš€ æˆ‘ä»¬å‘å¸ƒäº† InternVL2.5 ç³»åˆ—ï¼Œè¦†ç›–äº†ä»1Bå‚æ•°åˆ°78Bå‚æ•°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚[InternVL2_5-78B](https://huggingface.co/OpenGVLab/InternVL2_5-78B) æ˜¯é¦–ä¸ªåœ¨MMMU benchmarkä¸Šå¾—åˆ†è¶…è¿‡70çš„å¼€æºæ¨¡å‹ã€‚ è¿™äº›æ¨¡å‹å¯åœ¨ [HF é“¾æ¥](https://huggingface.co/collections/OpenGVLab/internvl-25-673e1019b66e2218f68d7c1c) ä¸­ä¸‹è½½ã€‚
- `2024/11/14`: æˆ‘ä»¬å‘å¸ƒäº† [MMPR](https://huggingface.co/datasets/OpenGVLab/MMPR)ï¼Œä¸€ä¸ªé«˜è´¨é‡ã€å¤§è§„æ¨¡çš„å¤šæ¨¡æ€æ¨ç†åå¥½æ•°æ®é›†ï¼Œä»¥åŠ [MPO](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo)ï¼Œä¸€ç§é«˜æ•ˆçš„åå¥½ä¼˜åŒ–ç®—æ³•ã€‚ç”±æ­¤è®­ç»ƒçš„æ¨¡å‹ [InternVL2-8B-MPO](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO) åœ¨ MathVista ä¸Šå–å¾—äº† 67.0 çš„å‡†ç¡®ç‡ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚é˜…æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2411.10442)ã€[é¡¹ç›®ä¸»é¡µ](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/) å’Œ [æ–‡æ¡£](https://internvl.readthedocs.io/en/latest/internvl2.0/preference_optimization.html)ã€‚
- `2024/10/21`: æˆ‘ä»¬å‘å¸ƒäº† Mini-InternVL ç³»åˆ—ã€‚è¿™äº›æ¨¡å‹åœ¨ä¿æŒæå°æ¨¡å‹ä½“ç§¯çš„åŒæ—¶å®ç°äº†å‡ºè‰²çš„æ€§èƒ½ï¼š4B æ¨¡å‹ä»…ç”¨ 5% çš„æ¨¡å‹å¤§å°ä¾¿è¾¾åˆ°äº† 90% çš„æ€§èƒ½ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ [é¡¹ç›®é¡µé¢](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/mini_internvl) å’Œ [æ–‡æ¡£](https://internvl.readthedocs.io/en/latest/internvl2.0/domain_adaptation.html)ã€‚
- `2024/08/01`: [Chartmimic](https://chartmimic.github.io/) å›¢é˜Ÿåœ¨ä»–ä»¬çš„åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº† InternVL2 ç³»åˆ—æ¨¡å‹ã€‚InternVL2-26B å’Œ 76B æ¨¡å‹åœ¨å¼€æºæ¨¡å‹ä¸­å–å¾—äº†å‰ä¸¤åçš„æˆç»©ï¼Œå…¶ä¸­ InternVL2-Llama3-76B æ¨¡å‹è¶…è¿‡äº† GeminiProVisionï¼Œå¹¶è¡¨ç°å‡ºä¸ Claude-3-opus ç›¸å½“çš„ç»“æœã€‚
- `2024/08/01`: InternVL2-Pro åœ¨ [CharXiv](https://charxiv.github.io/#leaderboard) æ•°æ®é›†ä¸­å®ç°äº†å¼€æºæ¨¡å‹ä¸­çš„ SOTA æ€§èƒ½ï¼Œä¹Ÿæ¯”éƒ¨åˆ†çŸ¥åé—­æºæ¨¡å‹å¦‚ GPT-4Vã€Gemini 1.5 Flashã€Claude 3 Sonnet å–å¾—äº†æ›´å¥½æˆç»©
- `2024/07/24`: [MLVU](https://github.com/JUNJIE99/MLVU)å›¢é˜Ÿåœ¨å®ƒä»¬çš„åŸºå‡†æµ‹è¯•ä¸­è¯„ä¼°äº†InternVL-1.5ã€‚åœ¨å¤šé¡¹é€‰æ‹©ä»»åŠ¡ä¸Šçš„å¹³å‡è¡¨ç°ä¸º50.4%ï¼Œè€Œåœ¨ç”Ÿæˆä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸º4.02ã€‚å¤šé¡¹é€‰æ‹©ä»»åŠ¡çš„è¡¨ç°åœ¨æ‰€æœ‰å¼€æºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­æ’åç¬¬ä¸€ã€‚
- `2024/07/04`: æˆ‘ä»¬å‘å¸ƒäº† InternVL2 ç³»åˆ—æ¨¡å‹ã€‚InternVL2-Pro åœ¨ MMMU åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† 62.0% çš„å‡†ç¡®ç‡ï¼Œå®ç°äº†ä¸ GPT-4o ç­‰é¢†å…ˆé—­æºå•†ä¸šæ¨¡å‹æ¯”è‚©çš„æ€§èƒ½ã€‚æ¨¡å‹æƒé‡å¯åœ¨ [HF é“¾æ¥](https://huggingface.co/collections/OpenGVLab/internvl-20-667d3961ab5eb12c7ed1463e) ä¸­ä¸‹è½½ã€‚

<details>
<summary>æ›´å¤š</summary>

- `2024/07/18`: InternVL2-40B åœ¨ [Video-MME](https://github.com/BradyFU/Video-MME) æ•°æ®é›†ä¸­å®ç°äº†å¼€æºæ¨¡å‹ä¸­çš„ SOTA æ€§èƒ½ï¼Œå½“è¾“å…¥ 16 å¸§æ—¶å¾—åˆ†ä¸º 61.2ï¼Œè¾“å…¥ 32 å¸§æ—¶å¾—åˆ†ä¸º 64.4ï¼Œå¤§å¹…é¢†å…ˆå…¶å®ƒå¼€æºæ¨¡å‹ï¼Œæ˜¯æœ€æ¥è¿‘ GPT-4o mini çš„å¼€æºæ¨¡å‹ã€‚
- `2024/07/18`: InternVL2-Pro åœ¨ [DocVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1) å’Œ [InfoVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=3) çš„åŸºå‡†æµ‹è¯•ä¸­å®ç°äº† SOTA æ€§èƒ½ã€‚
- `2024/06/19`: æˆ‘ä»¬æå‡ºäº† Needle In A Multimodal Haystack ([MM-NIAH](https://github.com/OpenGVLab/MM-NIAH))ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹æ¨¡å‹å…³äºé•¿å¤šæ¨¡æ€æ–‡æ¡£ç†è§£èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†ã€‚
- `2024/05/30`: æˆ‘ä»¬å‘å¸ƒäº† [ShareGPT-4o](https://sharegpt4o.github.io/)ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚æˆ‘ä»¬è®¡åˆ’å¼€æºä¸€æ‰¹ä½¿ç”¨ GPT-4o ç²¾å¿ƒæ ‡æ³¨çš„æ•°æ®ï¼ŒåŒ…æ‹¬ 200K æ¡å›¾åƒè¯¦ç»†æè¿°ã€10K æ¡è§†é¢‘è¯¦ç»†æè¿°ï¼Œä»¥åŠ 10K æ¡éŸ³é¢‘è¯¦ç»†æè¿°ã€‚
- `2024/05/29`: æˆ‘ä»¬å¼€æºäº† Mini-InternVL ç³»åˆ—ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªå¯¹è¯æ¨¡å‹ï¼š[Mini-InternVL-Chat-2B-V1-5](https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5) å’Œ [Mini-InternVL-Chat-4B-V1-5](https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-4B-V1-5)ã€‚è¿™äº›æ¨¡å‹åœ¨æå°çš„å°ºå¯¸ä¸‹å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼š2B æ¨¡å‹ä»¥ 8% çš„æ¨¡å‹å°ºå¯¸å®ç°äº† 80% çš„æ€§èƒ½ï¼Œ4B æ¨¡å‹ä»¥ 16% çš„æ¨¡å‹å°ºå¯¸å®ç°äº† 90% çš„æ€§èƒ½ã€‚æ›´å¤šç»†èŠ‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„[åšå®¢](https://internvl.github.io/blog/2024-05-25-Mini-InternVL-1.5/)ã€‚
- `2024/05/13`: InternVL 1.0 ç°åœ¨å¯ä»¥ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„ [æ–‡æœ¬ç¼–ç å™¨](https://huggingface.co/OpenGVLab/InternVL-14B-224px)ï¼Œæ”¯æŒå…¨çƒè¶…è¿‡ 110 ç§è¯­è¨€çš„å¤šè¯­è¨€ç”Ÿæˆã€‚è¯¦æƒ…è¯·çœ‹ [MuLan](https://github.com/mulanai/MuLan)ã€‚
- `2024/04/18`: InternVL-Chat-V1-5 å·²ç»åœ¨ [HuggingFace](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5) å‘å¸ƒï¼Œåœ¨ MMMUã€DocVQAã€ChartQAã€MathVista ç­‰å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æ¥è¿‘ GPT-4V å’Œ Gemini Proã€‚
- `2024/02/27`: InternVL å·²è¢« CVPR 2024 (Oral) æ¥æ”¶ï¼ğŸ‰
- `2024/02/21`: [InternVL-Chat-V1-2-Plus](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus) åœ¨ MathVistaï¼ˆ59.9ï¼‰ã€MMBenchï¼ˆ83.8ï¼‰å’Œ MMVPï¼ˆ58.7ï¼‰ä¸Šå®ç°äº† SOTA æ€§èƒ½ã€‚è¯¦æƒ…è¯·çœ‹æˆ‘ä»¬çš„[åšå®¢](https://internvl.github.io/blog/2024-02-21-InternVL-1.2/)ã€‚
- `2024/02/12`: InternVL-Chat-V1-2 å·²ç»å‘å¸ƒï¼Œå®ƒåœ¨ MMMU éªŒè¯é›†ä¸Šè¾¾åˆ°äº† 51.6ï¼Œåœ¨ MMBench æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº† 82.3ã€‚ æ›´å¤šä¿¡æ¯è¯·å‚è€ƒæˆ‘ä»¬çš„[åšå®¢](https://internvl.github.io/blog/2024-02-21-InternVL-1.2/)ä»¥åŠ [SFT æ•°æ®](./internvl_chat#prepare-training-datasets)ã€‚è¯¥æ¨¡å‹å·²ç»åœ¨ [HuggingFace](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2) å‘å¸ƒï¼Œè®­ç»ƒã€æµ‹è¯„çš„æ•°æ®å’Œè„šæœ¬å‡å·²å¼€æºã€‚
- `2024/01/24`: InternVL-Chat-V1-1 å·²ç»å‘å¸ƒï¼Œå®ƒæ”¯æŒä¸­æ–‡å¯¹è¯ï¼Œå¹¶å…·å¤‡å¼ºå¤§çš„ OCR èƒ½åŠ›ï¼Œè¯¦æƒ…è¯·çœ‹[è¿™é‡Œ](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1)ã€‚
- `2024/01/16`: æˆ‘ä»¬å‘å¸ƒäº† [å®šåˆ¶çš„ mmcv/mmsegmentation/mmdetection ä»£ç åº“](https://github.com/OpenGVLab/InternVL-MMDetSeg)ï¼Œé›†æˆäº† DeepSpeedï¼Œå¯ä»¥ç”¨äºè®­ç»ƒæ£€æµ‹å’Œåˆ†å‰²å¤§æ¨¡å‹ã€‚

</details>

## ä½¿ç”¨æ–‡æ¡£

### ğŸŒŸ **Get Started**

- **Installation**: ğŸŒ± [Installation Guide](https://internvl.readthedocs.io/en/latest/get_started/installation.html) | ğŸ“„ [requirements.txt](./requirements.txt)
- **Chat Data Format**: ğŸ“ [Meta File](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#meta-file) | âœï¸ [Text](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#pure-text-data) | ğŸ–¼ï¸ [Single-Image](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#single-image-data) | ğŸ–¼ï¸ğŸ–¼ï¸ [Multi-Image](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#multi-image-data) | ğŸ¥ [Video](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#video-data)
- **Local Chat Demo**: ğŸ¤– [Streamlit Demo](https://internvl.readthedocs.io/en/latest/get_started/local_chat_demo.html#streamlit-demo)
- **InternVL-Chat API**: ğŸŒ [InternVL2.5 API](https://internlm.intern-ai.org.cn/api/document)
- **Tutorials**: ğŸš€ [Enhancing InternVL2 on COCO Caption Using LoRA Fine-Tuning](https://internvl.readthedocs.io/en/latest/tutorials/coco_caption_finetune.html)

### ğŸ† **InternVL Family**

- **InternVL 2.5**: ğŸ“– [Intro](https://internvl.readthedocs.io/en/latest/internvl2.5/introduction.html) | âš¡ [Quick Start](https://internvl.readthedocs.io/en/latest/internvl2.5/quick_start.html) | âœ¨ [Finetune](https://internvl.readthedocs.io/en/latest/internvl2.5/finetune.html) | ğŸ“Š [Evaluate](https://internvl.readthedocs.io/en/latest/internvl2.5/evaluation.html) | ğŸ“¦ [Deploy](https://internvl.readthedocs.io/en/latest/internvl2.5/deployment.html) | ğŸ¯ [MPO](https://internvl.readthedocs.io/en/latest/internvl2.5/preference_optimization.html)
- **InternVL 2.0**: ğŸ“– [Intro](https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html) | âš¡ [Quick Start](https://internvl.readthedocs.io/en/latest/internvl2.0/quick_start.html) | âœ¨ [Finetune](https://internvl.readthedocs.io/en/latest/internvl2.0/finetune.html) | ğŸ“Š [Evaluate](https://internvl.readthedocs.io/en/latest/internvl2.0/evaluation.html) | ğŸ“¦ [Deploy](https://internvl.readthedocs.io/en/latest/internvl2.0/deployment.html) | ğŸ¯ [MPO](https://internvl.readthedocs.io/en/latest/internvl2.0/preference_optimization.html)
- **InternVL 1.5**: ğŸ“– [Intro](https://internvl.readthedocs.io/en/latest/internvl1.5/introduction.html) | âš¡ [Quick Start](https://internvl.readthedocs.io/en/latest/internvl1.5/quick_start.html) | âœ¨ [Finetune](https://internvl.readthedocs.io/en/latest/internvl1.5/finetune.html) | ğŸ“Š [Evaluate](https://internvl.readthedocs.io/en/latest/internvl1.5/evaluation.html) | ğŸ“¦ [Deploy](https://internvl.readthedocs.io/en/latest/internvl1.5/deployment.html)
- **InternVL 1.2**: ğŸ“– [Intro](https://internvl.readthedocs.io/en/latest/internvl1.2/introduction.html) | âš¡ [Quick Start](https://internvl.readthedocs.io/en/latest/internvl1.2/quick_start.html) | âœ¨ [Finetune](https://internvl.readthedocs.io/en/latest/internvl1.2/finetune.html) | ğŸ“Š [Evaluate](https://internvl.readthedocs.io/en/latest/internvl1.2/evaluation.html)
- **InternVL 1.1**: ğŸ“– [Intro](https://internvl.readthedocs.io/en/latest/internvl1.1/introduction.html) | âš¡ [Quick Start](https://internvl.readthedocs.io/en/latest/internvl1.1/quick_start.html) | ğŸ“Š [Evaluation](https://internvl.readthedocs.io/en/latest/internvl1.1/evaluation.html)
- **InternVL 1.0**: ğŸ–¼ï¸ [Classification](https://internvl.readthedocs.io/en/latest/internvl1.0/classification.html) | ğŸ“Š [CLIP-Benchmark](https://internvl.readthedocs.io/en/latest/internvl1.0/clip_benchmark.html) | ğŸ¨ [Segmentation](https://internvl.readthedocs.io/en/latest/internvl1.0/segmentation.html) | ğŸ’¬ [Chat-LLaVA](https://internvl.readthedocs.io/en/latest/internvl1.0/internvl_chat_llava.html) | âœ¨ [InternVL-G](https://internvl.readthedocs.io/en/latest/internvl1.0/internvl_g.html)

## æ¨¡å‹åº“

#### å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (InternVL 2.5)

<table>
  <tr>
    <th>Model Name</th>
    <th>Vision Part</th>
    <th>Language Part</th>
    <th>HF&nbsp;Link</th>
    <th>MS&nbsp;Link</th>
  </tr>
  <tr>
    <td>InternVL2_5-1B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">InternViT&#8209;300M&#8209;448px&#8209;V2_5</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct">Qwen2.5&#8209;0.5B&#8209;Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-1B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-1B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-2B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">InternViT-300M-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2_5-1_8b-chat">internlm2_5-1_8b-chat</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-2B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-2B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-4B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">InternViT-300M-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct">Qwen2.5-3B-Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-4B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-4B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-8B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">InternViT-300M-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2_5-7b-chat">internlm2_5-7b-chat</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-8B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-8B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-26B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5">InternViT-6B-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2_5-20b-chat">internlm2_5-20b-chat</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-26B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-26B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-38B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5">InternViT-6B-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct">Qwen2.5-32B-Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-38B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-38B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-78B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5">InternViT-6B-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2.5-72B-Instruct">Qwen2.5-72B-Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-78B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-78B">ğŸ¤– link</a></td>
  </tr>
</table>

<table>
  <tr>
    <th>Model Name</th>
    <th>Vision Part</th>
    <th>Language Part</th>
    <th>HF&nbsp;Link</th>
    <th>MS&nbsp;Link</th>
  </tr>
  <tr>
    <td>InternVL2_5-1B-MPO</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">InternViT&#8209;300M&#8209;448px&#8209;V2_5</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct">Qwen2.5&#8209;0.5B&#8209;Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-1B-MPO">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-1B-MPO">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-2B-MPO</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">InternViT-300M-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2_5-1_8b-chat">internlm2_5-1_8b-chat</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-2B-MPO">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-2B-MPO">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-4B-MPO</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">InternViT-300M-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2.5-3B-Instruct">Qwen2.5-3B-Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-4B-MPO">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-8B-MPO</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">InternViT-300M-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2_5-7b-chat">internlm2_5-7b-chat</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-8B-MPO">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-8B-MPO">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-26B-MPO</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5">InternViT-6B-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2_5-20b-chat">internlm2_5-20b-chat</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-26B-MPO">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-26B-MPO">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-38B-MPO</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5">InternViT-6B-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct">Qwen2.5-32B-Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-38B-MPO">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-38B-MPO">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2_5-78B-MPO</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5">InternViT-6B-448px-V2_5</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2.5-72B-Instruct">Qwen2.5-72B-Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2_5-78B-MPO">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2_5-78B-MPO">ğŸ¤– link</a></td>
  </tr>
</table>

#### å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (InternVL 2.0)

<table>
  <tr>
    <th>Model Name</th>
    <th>Vision Part</th>
    <th>Language Part</th>
    <th>HF&nbsp;Link</th>
    <th>MS&nbsp;Link</th>
  </tr>
  <tr>
    <td>InternVL2-1B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT-300M-448px</a></td>
    <td><a href="https://huggingface.co/Qwen/Qwen2-0.5B-Instruct">Qwen2-0.5B-Instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-1B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-1B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2-2B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT-300M-448px</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2-chat-1_8b">internlm2-chat-1-8b</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-2B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-2B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2-4B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT-300M-448px</a></td>
    <td><a href="https://huggingface.co/microsoft/Phi-3-mini-128k-instruct">Phi&#8209;3&#8209;mini&#8209;128k&#8209;instruct</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-4B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-4B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2-8B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">InternViT-300M-448px</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2_5-7b-chat">internlm2_5-7b-chat</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-8B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-8B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2-26B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT-6B-448px-V1-5</a></td>
    <td><a href="https://huggingface.co/internlm/internlm2-chat-20b">internlm2-chat-20b</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-26B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-26B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2-40B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;5</a></td>
    <td><a href="https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B">Nous&#8209;Hermes&#8209;2&#8209;Yi&#8209;34B</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-40B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-40B">ğŸ¤– link</a></td>
  </tr>
  <tr>
    <td>InternVL2&#8209;Llama3-76B</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">InternViT-6B-448px-V1-5</a></td>
    <td><a href="https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-70B">Hermesâ€‘2â€‘Thetaâ€‘<br>Llamaâ€‘3â€‘70B</a></td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL2-Llama3-76B">ğŸ¤– link</a></td>
  </tr>
</table>

#### å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (InternVL 1.0-1.5)

<table>
  <tr>
    <th>Model</th>
    <th>Date</th>
    <th>HF&nbsp;Link</th>
    <th>MS&nbsp;Link</th>
    <th>Note</th>
  </tr>
  <tr>
    <td>Mini&#8209;InternVL&#8209;Chat&#8209;4B&#8209;V1&#8209;5</td>
    <td>2024.05.28</td>
    <td><a href="https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-4B-V1-5">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-4B-V1-5">ğŸ¤– link</a></td>
    <td>ğŸš€ğŸš€ 16% çš„æ¨¡å‹å¤§å°, 90% çš„æ€§èƒ½</td>
  </tr>
  <tr>
    <td>Mini-InternVL-Chat-2B-V1-5</td>
    <td>2024.05.19</td>
    <td><a href="https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-2B-V1-5">ğŸ¤– link</a></td>
    <td>ğŸš€ 8% çš„æ¨¡å‹å¤§å°, 80% çš„æ€§èƒ½</td>
  </tr>
  <tr>
    <td>InternVL-Chat-V1-5</td>
    <td>2024.04.18</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-5">ğŸ¤– link</a></td>
    <td>æ”¯æŒ 4K å›¾åƒï¼›è¶…å¼ºçš„ OCR èƒ½åŠ›ï¼›åœ¨ MMMUã€DocVQAã€ChartQAã€MathVista ç­‰å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æ¥è¿‘ GPT-4V å’Œ Gemini Pro
  </tr>
  <tr>
    <td>InternVL-Chat-V1-2-Plus</td>
    <td>2024.02.21</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-2-Plus">ğŸ¤– link</a></td>
    <td>æ›´å¤šçš„ SFT æ•°æ®å’Œæ›´å¼ºçš„æ€§èƒ½</td>
  </tr>
  <tr>
    <td>InternVL-Chat-V1-2</td>
    <td>2024.02.11</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-2">ğŸ¤– link</a></td>
    <td>å°† LLM æ‰©å±•åˆ° 34B</td>
  </tr>
  <tr>
    <td>InternVL-Chat-V1-1</td>
    <td>2024.01.24</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-1">ğŸ¤– link</a></td>
    <td>æ”¯æŒä¸­æ–‡å’Œæ›´å¼ºçš„ OCR èƒ½åŠ›</td>
  </tr>
  <tr>
    <td>InternVL-Chat-19B</td>
    <td>2023.12.25</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B">ğŸ¤– link</a></td>
    <td>è‹±è¯­å¤šæ¨¡æ€å¯¹è¯</td>
  </tr>
  <tr>
    <td>InternVL-Chat-13B</td>
    <td>2023.12.25</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B">ğŸ¤– link</a></td>
    <td>è‹±è¯­å¤šæ¨¡æ€å¯¹è¯</td>
  </tr>
</table>

#### ç±» CLIP æ¨¡å‹ (InternVL 1.0-2.5)

<table>
  <tr>
    <th>Model</th>
    <th>Date</th>
    <th>HF&nbsp;Link</th>
    <th>MS&nbsp;Link</th>
    <th>Note</th>
  </tr>
  <tr>
    <td>InternViT-300M-448px-V2_5</td>
    <td>2024.12.05</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternViT-300M-448px-V2_5">ğŸ¤– link</a></td>
    <td>ğŸš€ğŸš€ ä¸€ä¸ªæ›´å¼ºçš„è½»é‡è§†è§‰ç¼–ç å™¨ (ğŸ”¥æ–°)</td>
  </tr>
  <tr>
    <td>InternViT-6B-448px-V2_5</td>
    <td>2024.12.05</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V2_5">ğŸ¤– link</a></td>
    <td>ğŸš€ğŸš€ æ‹¥æœ‰æ›´å¼ºçš„è§†è§‰ç‰¹å¾æå–èƒ½åŠ› (ğŸ”¥æ–°)</td>
  </tr>
  <tr>
    <td>InternViT-300M-448px</td>
    <td>2024.05.25</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-300M-448px">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternViT-300M-448px">ğŸ¤– link</a></td>
    <td>è’¸é¦çš„å°å‹è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå…·æœ‰ 300M å‚æ•°</td>
  </tr>
  <tr>
    <td>InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;5</td>
    <td>2024.04.20</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-5">ğŸ¤– link</a></td>
    <td>é€šè¿‡å¢é‡é¢„è®­ç»ƒæ”¯æŒåŠ¨æ€åˆ†è¾¨ç‡å’Œè¶…å¼ºçš„ OCR ç‰¹å¾æå–èƒ½åŠ›</td>
  </tr>
  <tr>
    <td>InternViT-6B-448px-V1-2</td>
    <td>2024.02.11</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-2">ğŸ¤– link</a></td>
    <td>é€šè¿‡å¢é‡é¢„è®­ç»ƒæ”¯æŒ 448 åˆ†è¾¨ç‡</td>
  </tr>
  <tr>
    <td>InternViT-6B-448px-V1-0</td>
    <td>2024.01.30</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-0">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-0">ğŸ¤– link</a></td>
    <td>é€šè¿‡å¢é‡é¢„è®­ç»ƒæ”¯æŒ 448 åˆ†è¾¨ç‡</td>
  </tr>
  <tr>
    <td>InternViT-6B-224px</td>
    <td>2023.12.22</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternViT-6B-224px">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternViT-6B-224px">ğŸ¤– link</a></td>
    <td>InternViT-6B çš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼Œæå–è‡ª InternVLâ€‘14Bâ€‘224px</td>
  </tr>
</table>

#### è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ (InternVL 1.0)

<table>
  <tr>
    <th>Model</th>
    <th>Date</th>
    <th>HF&nbsp;Link</th>
    <th>MS&nbsp;Link</th>
    <th>Note</th>
  </tr>
  <tr>
    <td>InternVL&#8209;14B&#8209;224px</td>
    <td>2023.12.22</td>
    <td><a href="https://huggingface.co/OpenGVLab/InternVL-14B-224px">ğŸ¤— link</a></td>
    <td><a href="https://modelscope.cn/models/OpenGVLab/InternVL-14B-224px">ğŸ¤– link</a></td>
    <td>è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ŒInternViT-6B + QLLaMAï¼Œå¯ä»¥ç”¨äºç±»ä¼¼ CLIP çš„å›¾æ–‡æ£€ç´¢</td>
  </tr>
</table>

## TODO åˆ—è¡¨

- [x] å‘å¸ƒ InternVL2.5 ç³»åˆ—çš„è®­ç»ƒ / è¯„ä¼°ä»£ç 
- [x] æ”¯æŒ liger kernels ä»¥èŠ‚çœæ˜¾å­˜
- [x] å‘å¸ƒ MPO çš„ä»£ç ã€æ¨¡å‹å’Œæ•°æ®
- [x] æ”¯æŒå¤šæ¨¡æ€ packed dataset
- [ ] æ”¯æŒ vLLM å’Œ Ollama
- [ ] åœ¨ Demo ä¸­æ”¯æŒè§†é¢‘å’Œ PDF è¾“å…¥
- [ ] å‘å¸ƒé›†æˆ VisionLLMv2 çš„ InternVL2
- [x] ä½¿ç”¨ readthedocs é‡æ–°æ„å»ºæ–‡æ¡£
- [x] æ”¯æŒä½¿ç”¨ LoRA å¾®è°ƒä¸åŒçš„ LLMs
- [x] å‘å¸ƒ InternVL2 çš„ `requirements.txt`
- [x] å‘å¸ƒ InternVL2 ç³»åˆ—çš„è®­ç»ƒ / è¯„ä¼°ä»£ç 
- [x] å‘å¸ƒ InternVL1.5 å’Œ InternVL2 çš„ Streamlit ç½‘é¡µ UI

## InternVL å¯ä»¥åšä»€ä¹ˆ?

<details>
  <summary>è§†è§‰æ„ŸçŸ¥ (ç‚¹å‡»å±•å¼€)</summary>

- çº¿æ€§æ¢é’ˆå›¾åƒåˆ†ç±» [\[æŸ¥çœ‹è¯¦æƒ…\]](./classification#-evaluation)

  ViT-22B uses the private JFT-3B dataset.

  | method              | #param | IN-1K | IN-ReaL | IN-V2 | IN-A  | IN-R  | IN-Sketch |
  | ------------------- | :----: | :---: | :-----: | :---: | :---: | :---: | :-------: |
  | OpenCLIP-G          |  1.8B  | 86.2  |  89.4   | 77.2  | 63.8  | 87.8  |   66.4    |
  | DINOv2-g            |  1.1B  | 86.5  |  89.6   | 78.4  | 75.9  | 78.8  |   62.5    |
  | EVA-01-CLIP-g       |  1.1B  | 86.5  |  89.3   | 77.4  | 70.5  | 87.7  |   63.1    |
  | MAWS-ViT-6.5B       |  6.5B  | 87.8  |    -    |   -   |   -   |   -   |     -     |
  | ViT-22B\*           | 21.7B  | 89.5  |  90.9   | 83.2  | 83.8  | 87.4  |     -     |
  | InternViT-6B (ours) |  5.9B  | 88.2  |  90.4   | 79.9  | 77.5  | 89.8  |   69.1    |

- è¯­ä¹‰åˆ†å‰² [\[æŸ¥çœ‹è¯¦æƒ…\]](./segmentation#-evaluation)

  | method                | decoder | #param (train/total) | crop size | mIoU         |
  | --------------------- | :-----: | :------------------: | :-------: | ------------ |
  | OpenCLIP-G (frozen)   | Linear  |     0.3M / 1.8B      |    512    | 39.3         |
  | ViT-22B (frozen)      | Linear  |     0.9M / 21.7B     |    504    | 34.6         |
  | InternViT-6B (frozen) | Linear  |     0.5M / 5.9B      |    504    | 47.2 (+12.6) |
  | ViT-22B (frozen)      | UperNet |     0.8B / 22.5B     |    504    | 52.7         |
  | InternViT-6B (frozen) | UperNet |     0.4B / 6.3B      |    504    | 54.9 (+2.2)  |
  | ViT-22B               | UperNet |    22.5B / 22.5B     |    504    | 55.3         |
  | InternViT-6B          | UperNet |     6.3B / 6.3B      |    504    | 58.9 (+3.6)  |

- é›¶æ ·æœ¬å›¾åƒåˆ†ç±» [\[æŸ¥çœ‹è¯¦æƒ…\]](./clip_benchmark#imagenet-variants-and-objectnet)

  | method            | IN-1K | IN-A  | IN-R  | IN-V2 | IN-Sketch | ObjectNet |
  | ----------------- | :---: | :---: | :---: | :---: | :-------: | :-------: |
  | OpenCLIP-G        | 80.1  | 69.3  | 92.1  | 73.6  |   68.9    |   73.0    |
  | EVA-02-CLIP-E+    | 82.0  | 82.1  | 94.5  | 75.7  |   71.6    |   79.6    |
  | ViT-22B\*         | 85.9  | 90.1  | 96.0  | 80.9  |     -     |   87.6    |
  | InternVL-C (ours) | 83.2  | 83.8  | 95.5  | 77.3  |   73.9    |   80.6    |

- å¤šè¯­è¨€é›¶æ ·æœ¬å›¾åƒåˆ†ç±» [\[æŸ¥çœ‹è¯¦æƒ…\]](./clip_benchmark#multilingual-imagenet-1k)

  EN: English, ZH: Chinese, JP: Japanese, Ar: Arabic, IT: Italian

  | method            | IN-1K (EN) | IN-1K (ZH) | IN-1K (JP) | IN-1K (AR) | IN-1K (IT) |
  | ----------------- | :--------: | :--------: | :--------: | :--------: | :--------: |
  | Taiyi-CLIP-ViT-H  |     -      |    54.4    |     -      |     -      |     -      |
  | WuKong-ViT-L-G    |     -      |    57.5    |     -      |     -      |     -      |
  | CN-CLIP-ViT-H     |     -      |    59.6    |     -      |     -      |     -      |
  | AltCLIP-ViT-L     |    74.5    |    59.6    |     -      |     -      |     -      |
  | EVA-02-CLIP-E+    |    82.0    |     -      |     -      |     -      |    41.2    |
  | OpenCLIP-XLM-R-H  |    77.0    |    55.7    |    53.1    |    37.0    |    56.8    |
  | InternVL-C (ours) |    83.2    |    64.5    |    61.5    |    44.9    |    65.7    |

- é›¶æ ·æœ¬è§†é¢‘åˆ†ç±»

  | method            | #frame | K400  | K600  | K700  |
  | ----------------- | :----: | :---: | :---: | :---: |
  | OpenCLIP-G        |   1    | 65.9  | 66.1  | 59.2  |
  | EVA-02-CLIP-E+    |   1    | 69.8  | 69.3  | 63.4  |
  | InternVL-C (ours) |   1    | 71.0  | 71.3  | 65.7  |
  | ViCLIP            |   8    | 75.7  | 73.5  | 66.4  |
  | InternVL-C (ours) |   8    | 79.4  | 78.8  | 71.5  |

</details>

<details>
  <summary>è·¨æ¨¡æ€æ£€ç´¢ (ç‚¹å‡»å±•å¼€)</summary>

- è‹±è¯­é›¶æ ·æœ¬å›¾æ–‡æ£€ç´¢ [\[æŸ¥çœ‹è¯¦æƒ…\]](./clip_benchmark#flickr30k--coco)

  <table>
    <tr align=center>
        <td rowspan="3" align=left><b>model</b></td>
        <td colspan="6" align=center><b>Flickr30K</b></td>
        <td colspan="6" align=center><b>COCO</b></td>
        <td rowspan="3" align=center><b>avg</b></td>
    </tr>
     <tr align=center>
        <td colspan="3" align=center><b>image-to-text</b></td>
        <td colspan="3" align=center><b>text-to-image</b></td>
         <td colspan="3" align=center><b>image-to-text</b></td>
        <td colspan="3" align=center><b>text-to-image</b></td>
     </tr>
     <tr>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
     </tr>
  <tr align=center>
        <td align=left>OpenCLIP-G</td>
        <td>92.9</td>
        <td>99.3</td>
        <td>99.8</td>
        <td>79.5</td>
        <td>95.0</td>
        <td>97.1</td>
        <td>67.3</td>
        <td>86.9</td>
        <td>92.6</td>
        <td>51.4</td>
        <td>74.9</td>
        <td>83.0</td>
        <td>85.0</td>
     </tr>
  <tr align=center>
        <td align=left>EVA-02-CLIP-E+</td>
        <td>93.9</td>
        <td>99.4</td>
        <td>99.8</td>
        <td>78.8</td>
        <td>94.2</td>
        <td>96.8</td>
        <td>68.8</td>
        <td>87.8</td>
        <td>92.8</td>
        <td>51.1</td>
        <td>75.0</td>
        <td>82.7</td>
        <td>85.1</td>
     </tr>
    <tr align=center>
        <td align=left>EVA-CLIP-8B</td>
        <td>95.6</td>
        <td>99.6</td>
        <td>99.9</td>
        <td>80.8</td>
        <td>95.5</td>
        <td>97.6</td>
        <td>70.3</td>
        <td>89.3</td>
        <td>93.9</td>
        <td>53.0</td>
        <td>76.0</td>
        <td>83.4</td>
        <td>86.2</td>
     </tr>
  <tr align=center>
        <td align=left>InternVL-C (ours)</td>
        <td>94.7</td>
        <td>99.6</td>
        <td>99.9</td>
        <td>81.7</td>
        <td>96.0</td>
        <td>98.2</td>
        <td>70.6</td>
        <td>89.0</td>
        <td>93.5</td>
        <td>54.1</td>
        <td>77.3</td>
        <td>84.6</td>
        <td>86.6</td>
     </tr>
  <tr align=center>
        <td align=left>InternVL-G (ours)</td>
        <td>95.7</td>
        <td>99.7</td>
        <td>99.9</td>
        <td>85.0</td>
        <td>97.0</td>
        <td>98.6</td>
        <td>74.9</td>
        <td>91.3</td>
        <td>95.2</td>
        <td>58.6</td>
        <td>81.3</td>
        <td>88.0</td>
        <td>88.8</td>
     </tr>

  </table>

- ä¸­æ–‡é›¶æ ·æœ¬å›¾æ–‡æ£€ç´¢ [\[æŸ¥çœ‹è¯¦æƒ…\]](./clip_benchmark#flickr30k-cn--coco-cn)

  <table>
    <tr  align=center>
        <td rowspan="3" align=left><b>model</b></td>
        <td colspan="6" align=center><b>Flickr30K-CN</b></td>
        <td colspan="6" align=center><b>COCO-CN</b></td>
        <td rowspan="3" align=center><b>avg</b></td>

  </tr>
     <tr  align=center>
        <td colspan="3" align=center><b>image-to-text</b></td>
        <td colspan="3" align=center><b>text-to-image</b></td>
         <td colspan="3" align=center><b>image-to-text</b></td>
        <td colspan="3" align=center><b>text-to-image</b></td>
     </tr>
     <tr>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
     </tr>

  <tr align=center>
        <td align=left>CN-CLIP-ViT-H</td>
        <td>81.6</td>
        <td>97.5</td>
        <td>98.8</td>
        <td>71.2</td>
        <td>91.4</td>
        <td>95.5</td>
        <td>63.0</td>
        <td>86.6</td>
        <td>92.9</td>
        <td>69.2</td>
        <td>89.9</td>
        <td>96.1</td>
        <td>86.1</td>
     </tr>

  <tr align=center>
        <td align=left>OpenCLIP-XLM-R-H</td>
        <td>86.1</td>
        <td>97.5</td>
        <td>99.2</td>
        <td>71.0</td>
        <td>90.5</td>
        <td>94.9</td>
        <td>70.0</td>
        <td>91.5</td>
        <td>97.0</td>
        <td>66.1</td>
        <td>90.8</td>
        <td>96.0</td>
        <td>87.6</td>
     </tr>

  <tr align=center>
        <td align=left>InternVL-C (ours)</td>
        <td>90.3</td>
        <td>98.8</td>
        <td>99.7</td>
        <td>75.1</td>
        <td>92.9</td>
        <td>96.4</td>
        <td>68.8</td>
        <td>92.0</td>
        <td>96.7</td>
        <td>68.9</td>
        <td>91.9</td>
        <td>96.5</td>
        <td>89.0</td>
     </tr>
  <tr align=center>
        <td align=left>InternVL-G (ours)</td>
        <td>92.9</td>
        <td>99.4</td>
        <td>99.8</td>
        <td>77.7</td>
        <td>94.8</td>
        <td>97.3</td>
        <td>71.4</td>
        <td>93.9</td>
        <td>97.7</td>
        <td>73.8</td>
        <td>94.4</td>
        <td>98.1</td>
        <td>90.9</td>
     </tr>

  </table>

- å¤šè¯­è¨€é›¶æ ·æœ¬å›¾æ–‡å¯¹æ£€ç´¢ [\[æŸ¥çœ‹è¯¦æƒ…\]](./clip_benchmark#xtd)

  | method            |  EN   |  ES   |  FR   |  ZH   |  IT   |  KO   |  RU   |  JP   | average |
  | ----------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :-----: |
  | AltCLIP           | 95.4  | 94.1  | 92.9  | 95.1  | 94.2  | 94.4  | 91.8  | 91.7  |  93.7   |
  | OpenCLIP-XLM-R-H  | 97.3  | 96.1  | 94.5  | 94.7  | 96.0  | 90.2  | 93.9  | 94.0  |  94.6   |
  | InternVL-C (ours) | 97.3  | 95.7  | 95.1  | 95.6  | 96.0  | 92.2  | 93.3  | 95.5  |  95.1   |
  | InternVL-G (ours) | 98.6  | 97.7  | 96.5  | 96.7  | 96.9  | 95.1  | 94.8  | 96.1  |  96.6   |

</details>

<details>
  <summary>å¤šæ¨¡æ€å¯¹è¯</summary>

</details>

## ä½¿ç”¨ HuggingFace å¿«é€Ÿå¼€å§‹

<details>
  <summary>ä½¿ç”¨ InternViT-6B æå–è§†è§‰ç‰¹å¾ (ç‚¹å‡»å±•å¼€)</summary>

```python
import torch
from PIL import Image
from transformers import AutoModel, CLIPImageProcessor

model = AutoModel.from_pretrained(
    'OpenGVLab/InternViT-6B-448px-V2_5',
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).cuda().eval()

image = Image.open('./examples/image1.jpg').convert('RGB')

image_processor = CLIPImageProcessor.from_pretrained('OpenGVLab/InternViT-6B-448px-V1-5')

pixel_values = image_processor(images=image, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()

outputs = model(pixel_values)
```

</details>

<details>
  <summary>ä½¿ç”¨ InternVL-C(ontrastive) å’Œ InternVL-G(enerative) è¿›è¡Œè·¨æ¨¡æ€æ£€ç´¢ (ç‚¹å‡»å±•å¼€)</summary>

```python
import torch
from PIL import Image
from transformers import AutoModel, CLIPImageProcessor
from transformers import AutoTokenizer


model = AutoModel.from_pretrained(
    'OpenGVLab/InternVL-14B-224px',
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).cuda().eval()

image_processor = CLIPImageProcessor.from_pretrained('OpenGVLab/InternVL-14B-224px')

tokenizer = AutoTokenizer.from_pretrained(
    'OpenGVLab/InternVL-14B-224px', use_fast=False, add_eos_token=True)
tokenizer.pad_token_id = 0  # set pad_token_id to 0

images = [
    Image.open('./examples/image1.jpg').convert('RGB'),
    Image.open('./examples/image2.jpg').convert('RGB'),
    Image.open('./examples/image3.jpg').convert('RGB')
]
prefix = 'summarize:'
texts = [
    prefix + 'a photo of a red panda',  # English
    prefix + 'ä¸€å¼ ç†ŠçŒ«çš„ç…§ç‰‡',  # Chinese
    prefix + 'äºŒåŒ¹ã®çŒ«ã®å†™çœŸ'  # Japanese
]

pixel_values = image_processor(images=images, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()
input_ids = tokenizer(texts, return_tensors='pt', max_length=80,
                      truncation=True, padding='max_length').input_ids.cuda()

# InternVL-C
logits_per_image, logits_per_text = model(
    image=pixel_values, text=input_ids, mode='InternVL-C')
probs = logits_per_image.softmax(dim=-1)
# tensor([[9.9609e-01, 5.2185e-03, 6.0070e-08],
#         [2.2949e-02, 9.7656e-01, 5.9903e-06],
#         [3.2932e-06, 7.4863e-05, 1.0000e+00]], device='cuda:0',
#        dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)

# InternVL-G
logits_per_image, logits_per_text = model(
    image=pixel_values, text=input_ids, mode='InternVL-G')
probs = logits_per_image.softmax(dim=-1)
# tensor([[9.9609e-01, 3.1738e-03, 3.6322e-08],
#         [8.6060e-03, 9.9219e-01, 2.8759e-06],
#         [1.7583e-06, 3.1233e-05, 1.0000e+00]], device='cuda:0',
#        dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)

# please set add_eos_token to False for generation
tokenizer.add_eos_token = False
image = Image.open('./examples/image1.jpg').convert('RGB')
pixel_values = image_processor(images=image, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()

tokenized = tokenizer("English caption:", return_tensors='pt')
pred = model.generate(
    pixel_values=pixel_values,
    input_ids=tokenized.input_ids.cuda(),
    attention_mask=tokenized.attention_mask.cuda(),
    num_beams=5,
    min_new_tokens=8,
)
caption = tokenizer.decode(pred[0].cpu(), skip_special_tokens=True).strip()
# English caption: a red panda sitting on top of a wooden platform
```

</details>

<details>
  <summary>ä½¿ç”¨ InternVL 2.5 è¿›è¡Œå¤šæ¨¡æ€å¯¹è¯ (ç‚¹å‡»å±•å¼€)</summary>

è¿™é‡Œæˆ‘ä»¬ä»¥è¾ƒå°çš„ `OpenGVLab/InternVL2_5-8B` ä¸ºä¾‹ï¼š

```python
import numpy as np
import torch
import torchvision.transforms as T
from decord import VideoReader, cpu
from PIL import Image
from torchvision.transforms.functional import InterpolationMode
from transformers import AutoModel, AutoTokenizer

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)

def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform

def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio

def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # calculate the existing image aspect ratio
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # find the closest aspect ratio to the target
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # calculate the target width and height
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # resize the image
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # split the image
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images

def load_image(image_file, input_size=448, max_num=12):
    image = Image.open(image_file).convert('RGB')
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(image) for image in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values

# If you have an 80G A100 GPU, you can put the entire model on a single GPU.
# Otherwise, you need to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.
path = 'OpenGVLab/InternVL2_5-8B'
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).eval().cuda()
tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)

# set the max number of tiles in `max_num`
pixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()
generation_config = dict(max_new_tokens=1024, do_sample=False)

# pure-text conversation (çº¯æ–‡æœ¬å¯¹è¯)
question = 'Hello, who are you?'
response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)
print(f'User: {question}\nAssistant: {response}')

question = 'Can you tell me a story?'
response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)
print(f'User: {question}\nAssistant: {response}')

# single-image single-round conversation (å•å›¾å•è½®å¯¹è¯)
question = '<image>\nPlease describe the image shortly.'
response = model.chat(tokenizer, pixel_values, question, generation_config)
print(f'User: {question}\nAssistant: {response}')

# single-image multi-round conversation (å•å›¾å¤šè½®å¯¹è¯)
question = '<image>\nPlease describe the image in detail.'
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)
print(f'User: {question}\nAssistant: {response}')

question = 'Please write a poem according to the image.'
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)
print(f'User: {question}\nAssistant: {response}')

# multi-image multi-round conversation, combined images (å¤šå›¾å¤šè½®å¯¹è¯ï¼Œæ‹¼æ¥å›¾åƒ)
pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()
pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()
pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)

question = '<image>\nDescribe the two images in detail.'
response, history = model.chat(tokenizer, pixel_values, question, generation_config,
                               history=None, return_history=True)
print(f'User: {question}\nAssistant: {response}')

question = 'What are the similarities and differences between these two images.'
response, history = model.chat(tokenizer, pixel_values, question, generation_config,
                               history=history, return_history=True)
print(f'User: {question}\nAssistant: {response}')

# multi-image multi-round conversation, separate images (å¤šå›¾å¤šè½®å¯¹è¯ï¼Œç‹¬ç«‹å›¾åƒ)
pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()
pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()
pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)
num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]

question = 'Image-1: <image>\nImage-2: <image>\nDescribe the two images in detail.'
response, history = model.chat(tokenizer, pixel_values, question, generation_config,
                               num_patches_list=num_patches_list,
                               history=None, return_history=True)
print(f'User: {question}\nAssistant: {response}')

question = 'What are the similarities and differences between these two images.'
response, history = model.chat(tokenizer, pixel_values, question, generation_config,
                               num_patches_list=num_patches_list,
                               history=history, return_history=True)
print(f'User: {question}\nAssistant: {response}')

# batch inference, single image per sample (å•å›¾æ‰¹å¤„ç†)
pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()
pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()
num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]
pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)

questions = ['<image>\nDescribe the image in detail.'] * len(num_patches_list)
responses = model.batch_chat(tokenizer, pixel_values,
                             num_patches_list=num_patches_list,
                             questions=questions,
                             generation_config=generation_config)
for question, response in zip(questions, responses):
    print(f'User: {question}\nAssistant: {response}')

# video multi-round conversation (è§†é¢‘å¤šè½®å¯¹è¯)
def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):
    if bound:
        start, end = bound[0], bound[1]
    else:
        start, end = -100000, 100000
    start_idx = max(first_idx, round(start * fps))
    end_idx = min(round(end * fps), max_frame)
    seg_size = float(end_idx - start_idx) / num_segments
    frame_indices = np.array([
        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))
        for idx in range(num_segments)
    ])
    return frame_indices

def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):
    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)
    max_frame = len(vr) - 1
    fps = float(vr.get_avg_fps())

    pixel_values_list, num_patches_list = [], []
    transform = build_transform(input_size=input_size)
    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)
    for frame_index in frame_indices:
        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')
        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)
        pixel_values = [transform(tile) for tile in img]
        pixel_values = torch.stack(pixel_values)
        num_patches_list.append(pixel_values.shape[0])
        pixel_values_list.append(pixel_values)
    pixel_values = torch.cat(pixel_values_list)
    return pixel_values, num_patches_list

video_path = './examples/red-panda.mp4'
pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)
pixel_values = pixel_values.to(torch.bfloat16).cuda()
video_prefix = ''.join([f'Frame-{i+1}: <image>\n' for i in range(len(num_patches_list))])
question = video_prefix + 'What is the red panda doing?'
# Frame1: <image>\nFrame2: <image>\n...\nFrame8: <image>\n{question}
response, history = model.chat(tokenizer, pixel_values, question, generation_config,
                               num_patches_list=num_patches_list, history=None, return_history=True)
print(f'User: {question}\nAssistant: {response}')

question = 'Describe this video in detail.'
response, history = model.chat(tokenizer, pixel_values, question, generation_config,
                               num_patches_list=num_patches_list, history=history, return_history=True)
print(f'User: {question}\nAssistant: {response}')
```

</details>

## è®¸å¯è¯

æœ¬é¡¹ç›®ä»¥ [MIT è®¸å¯è¯](LICENSE) å‘å¸ƒã€‚é¡¹ç›®ä¸­çš„éƒ¨åˆ†ä»£ç å’Œæ¨¡å‹æ¥è‡ªå…¶å®ƒæ¥æºï¼Œå—å…¶åŸå§‹è®¸å¯è¯çš„çº¦æŸã€‚

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­å‘ç°æœ¬é¡¹ç›®æœ‰ç”¨ï¼Œè¯·è€ƒè™‘å¼•ç”¨ï¼š

```BibTeX
@article{chen2024expanding,
  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{wang2024mpo,
  title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization},
  author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2411.10442},
  year={2024}
}
@article{gao2024mini,
  title={Mini-InternVL: a flexible-transfer pocket multi-modal model with 5\% parameters and 90\% performance},
  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},
  journal={Visual Intelligence},
  volume={2},
  number={1},
  pages={1--17},
  year={2024},
  publisher={Springer}
}
@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={Science China Information Sciences},
  volume={67},
  number={12},
  pages={220101},
  year={2024},
  publisher={Springer}
}
@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}
```

## è‡´è°¢

InternVL çš„ä»£ç æ„å»ºå‚è€ƒäº†ä»¥ä¸‹çš„é¡¹ç›®: [OpenAI CLIP](https://github.com/openai/CLIP)ã€[Open CLIP](https://github.com/mlfoundations/open_clip)ã€[CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark)ã€[EVA](https://github.com/baaivision/EVA/tree/master)ã€[InternImage](https://github.com/OpenGVLab/InternImage)ã€[ViT-Adapter](https://github.com/czczup/ViT-Adapter)ã€[MMSegmentation](https://github.com/open-mmlab/mmsegmentation)ã€[Transformers](https://github.com/huggingface/transformers)ã€[DINOv2](https://github.com/facebookresearch/dinov2)ã€[BLIP-2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)ã€[Qwen-VL](https://github.com/QwenLM/Qwen-VL/tree/master/eval_mm)å’Œ [LLaVA-1.5](https://github.com/haotian-liu/LLaVA)ï¼Œæ„Ÿè°¢è¿™äº›æ°å‡ºçš„å·¥ä½œã€‚

______________________________________________________________________

æ‰«æä¸‹æ–¹äºŒç»´ç ï¼ŒåŠ å…¥æˆ‘ä»¬çš„é¡¹ç›®å¾®ä¿¡ç¾¤ã€‚

<p align="center"><img width="300" alt="image" src="https://github.com/user-attachments/assets/f776df09-ebba-4fd5-80c2-fec4ff1518be"></p>
