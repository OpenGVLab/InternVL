# <img width="60" alt="image" src="https://github.com/OpenGVLab/InternVL/assets/47669167/7037290e-f474-4d11-b90f-1d8316087bf8"> InternVLå®¶æ—ï¼šé€šè¿‡å¼€æºç»„ä»¶ç¼©å°ä¸å•†ä¸šå¤šæ¨¡æ€æ¨¡å‹çš„å·®è· â€”â€” GPT-4Vçš„å¼€æºæ›¿ä»£æ–¹æ¡ˆ

[\[ğŸ“– è‹±æ–‡ç‰ˆæœ¬\]](./README.md) [\[ğŸ†• åšå®¢\]](https://internvl.github.io/blog/)  [\[ğŸ“œ InternVL 1.0 è®ºæ–‡\]](https://arxiv.org/abs/2312.14238)  [\[ğŸ“œ InternVL 1.5 æŠ€æœ¯æŠ¥å‘Š\]](https://arxiv.org/abs/2404.16821)  [\[ğŸ—¨ï¸ Chat Demo\]](https://internvl.opengvlab.com/) [\[ğŸ¤— HuggingFace Demo\]](https://huggingface.co/spaces/OpenGVLab/InternVL)

[\[ğŸš€ å¿«é€Ÿå¼€å§‹\]](#ä½¿ç”¨-huggingface-å¿«é€Ÿå¼€å§‹)  [\[ğŸŒ Community-hosted API\]](https://rapidapi.com/adushar1320/api/internvl-chat)  [\[ğŸ“– ä¸­æ–‡è§£è¯»\]](https://zhuanlan.zhihu.com/p/675877376)

<a href="https://trendshift.io/repositories/9803" target="_blank"><img src="https://trendshift.io/api/badge/repositories/9803" alt="OpenGVLab%2FInternVL | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

## æœ€æ–°æ¶ˆæ¯ğŸš€ğŸš€ğŸš€
- `2024/06/19`: ğŸš€ æˆ‘ä»¬æå‡ºäº† Needle In A Multimodal Haystack ([MM-NIAH](https://github.com/OpenGVLab/MM-NIAH))ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹æ¨¡å‹å…³äºé•¿å¤šæ¨¡æ€æ–‡æ¡£ç†è§£èƒ½åŠ›çš„è¯„æµ‹åŸºå‡†ã€‚**å®éªŒç»“æœè¡¨æ˜ï¼ŒGemini-1.5åœ¨åŒ…å«å›¾åƒé’ˆçš„æ•°æ®ä¸Šçš„æ€§èƒ½å¹¶ä¸æ¯”ä¹±çŒœçš„æ€§èƒ½è¦å¥½ã€‚**
- `2024/06/04`: InternVL 1.5 åœ¨ [Video-MME](https://github.com/BradyFU/Video-MME) æ•°æ®é›†çš„ Image MLLM ç±»åˆ«ä¸­å®ç°äº†SOTAçš„æ€§èƒ½ï¼Œå±•ç¤ºäº†åœ¨å¤šå›¾åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œè¶…è¿‡äº†è®¸å¤šä¸“é—¨çš„ Video MLLMï¼Œå¹¶æ¥è¿‘å¼€æºSOTAè§†é¢‘æ¨¡å‹ LLaVA-Next-Videoã€‚
- `2024/05/29`: ğŸš€ æˆ‘ä»¬å¼€æºäº† Mini-InternVL-Chat ç³»åˆ—ï¼Œç›®å‰åŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªæ¨¡å‹ï¼š[Mini-InternVL-Chat-2B-V1-5](https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5) å’Œ [Mini-InternVL-Chat-4B-V1-5](https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-4B-V1-5)ã€‚æˆ‘ä»¬çš„å°æ¨¡å‹åœ¨æå°çš„å°ºå¯¸ä¸‹å®ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼š2Bæ¨¡å‹ä»…ä»¥8%çš„æ¨¡å‹å°ºå¯¸å®ç°äº†80%çš„æ€§èƒ½ï¼Œ4Bæ¨¡å‹ä»¥16%çš„æ¨¡å‹å°ºå¯¸å®ç°äº†90%çš„æ€§èƒ½ã€‚æ›´å¤šç»†èŠ‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„[åšå®¢](https://internvl.github.io/blog/2024-05-25-Mini-InternVL-1.5/)ã€‚
- `2024/05/28`: æ„Ÿè°¢ [lmdeploy](https://github.com/InternLM/lmdeploy) å›¢é˜Ÿæä¾›çš„AWQé‡åŒ–æ”¯æŒã€‚4-bitæ¨¡å‹å‘å¸ƒåœ¨ [OpenGVLab/InternVL-Chat-V1-5-AWQ](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-AWQ)ã€‚
- `2024/05/13`: ğŸ”¥ InternVL ç°åœ¨å¯ä»¥ä½œä¸ºæ‰©æ•£æ¨¡å‹çš„ [æ–‡æœ¬ç¼–ç å™¨](https://huggingface.co/OpenGVLab/InternVL-14B-224px)ï¼Œæ”¯æŒå…¨çƒè¶…è¿‡110ç§è¯­è¨€çš„å¤šè¯­è¨€ç”Ÿæˆã€‚è¯¦æƒ…è¯·çœ‹ [MuLan](https://github.com/mulanai/MuLan)ã€‚
- `2024/04/28`: æˆ‘ä»¬å‘å¸ƒäº† InternVL-Chat-V1-5 çš„ INT8 é‡åŒ–ç‰ˆæœ¬ï¼Œè¯¦ç»†è¯·çœ‹ [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-Int8)ã€‚
- `2024/04/28`: æˆ‘ä»¬åœ¨ Infographics VQA çš„åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA æ€§èƒ½ï¼ˆ75.74ï¼‰ï¼Œè¯¦æƒ…è¯·çœ‹ [here](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=3)ã€‚
- `2024/04/18`: InternVL-Chat-V1-5 å·²ç»åœ¨ [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5) å‘å¸ƒï¼Œåœ¨MMMUã€DocVQAã€ChartQAã€MathVistaç­‰å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ€§èƒ½æ¥è¿‘GPT-4Vå’ŒGemini Proã€‚
- `2024/02/27`: InternVL è¢« CVPR 2024 æ¥æ”¶ï¼ğŸ‰
- `2024/02/24`: InternVL-Chat æ¨¡å‹å·²ç»æ¥å…¥ [VLMEvalKit](https://github.com/open-compass/VLMEvalKit)ã€‚
- `2024/02/21`: [InternVL-Chat-V1-2-Plus](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus) åœ¨ MathVistaï¼ˆ59.9ï¼‰ã€MMBenchï¼ˆ83.8ï¼‰å’ŒMMVPï¼ˆ58.7ï¼‰ä¸Šè¾¾åˆ°äº†SOTAæ€§èƒ½ã€‚è¯¦æƒ…è¯·å‚è§æˆ‘ä»¬çš„ [blog](<[BLOG.md](https://internvl.github.io/blog/2024-02-21-InternVL-1.2/)>)ã€‚
- `2024/02/12`: InternVL-Chat-V1-2 å·²ç»å‘å¸ƒã€‚å®ƒåœ¨MMMUéªŒè¯é›†ä¸Šè¾¾åˆ°äº†51.6çš„åˆ†æ•°ï¼Œåœ¨MMBenchæµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†82.3çš„åˆ†æ•°ã€‚ æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [blog](<[BLOG.md](https://internvl.github.io/blog/2024-02-21-InternVL-1.2/)>)ã€[SFT data](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets) æˆ–è€…å°è¯•æˆ‘ä»¬çš„ [demo](https://internvl.opengvlab.com/)ã€‚è¯¥æ¨¡å‹å·²ç»åœ¨ [HuggingFace](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2) å‘å¸ƒï¼Œè®­ç»ƒã€æµ‹è¯„çš„æ•°æ®å’Œè„šæœ¬å‡å·²å¼€æºã€‚
- `2024/02/04`: [InternVL-Chat-V1-1](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1) åœ¨ [MMVP](https://github.com/tsb0601/MMVP) ä¸Šè¾¾åˆ°äº† 44.67 çš„å¾—åˆ†ï¼Œé«˜äºGPT-4Vï¼
- `2024/01/27`: æˆ‘ä»¬å‘å¸ƒäº†448åˆ†è¾¨ç‡çš„æ¨¡å‹ï¼Œåœ¨MMBenchçš„éªŒè¯é›†ä¸Šè¾¾åˆ°äº†76.6çš„åˆ†æ•°ï¼Œè¯¦æƒ…è¯·çœ‹ [here](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#-evaluation-chinese-models)ã€‚
- `2024/01/24`: InternVL-Chat-V1-1 å·²ç»å‘å¸ƒï¼Œå®ƒæ”¯æŒä¸­æ–‡ï¼Œå¹¶ä¸”æœ‰å¼ºå¤§çš„OCRèƒ½åŠ›ï¼Œè¯¦æƒ…è¯·çœ‹ [here](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1) æˆ–è€…å°è¯•æˆ‘ä»¬çš„ [demo](https://internvl.opengvlab.com/)ã€‚
- `2024/01/16`: æˆ‘ä»¬å‘å¸ƒäº† [å®šåˆ¶åŒ–çš„ mmcv/mmsegmentation/mmdetection code](https://github.com/OpenGVLab/InternVL-MMDetSeg)ï¼Œé›†æˆäº†DeepSpeedï¼Œå¯ä»¥ç”¨äºè®­ç»ƒç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²å¤§æ¨¡å‹ã€‚

## æ–‡æ¡£

- å®‰è£…

  - å¦‚ä½•æ­å»ºè¿è¡Œç¯å¢ƒ?  [\[link\]](./INSTALLATION.md)

- è®­ç»ƒæˆ–è€…å¾®è°ƒ

  - å¦‚ä½•å¤ç° InternVL-Chat-V1-2 çš„SFTé˜¶æ®µ? [\[link\]](./internvl_chat#start-training)
  - å¦‚ä½•åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒ InternVL-Chat-V1-2? [\[link\]](./document/how_to_finetune_internvl_chat_v1_2_on_a_custom_dataset.md)
  - å¦‚ä½•åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šå¾®è°ƒ Mini-InternVL-Chat ç³»åˆ—? [\[link\]](./document/How_to_finetune_mini_internvl_chat_v1_5_on_a_custom_dataset.md)

- Benchmark æµ‹è¯„

  > ç”±äºæ­¤ä»£ç åº“ä¸ VLMEvalKit ä¹‹é—´å­˜åœ¨ç»†å¾®çš„å®ç°å·®å¼‚ï¼Œåœ¨æµ‹è¯•åŒä¸€æ¨¡å‹æ—¶ï¼Œæ€§èƒ½æŒ‡æ ‡å¯èƒ½ä¼šå‡ºç°è½»å¾®å·®å¼‚ã€‚

  - å¦‚ä½•è¯„æµ‹ InternVL-Chat-V1-5? [\[link\]](./document/how_to_evaluate_internvl_chat_v1_5.md)
  - å¦‚ä½•ä½¿ç”¨ VLMEvalKit è¯„æµ‹ InternVL-Chat-V1-5? (æ¨è) [\[link\]](./document/how_to_evaluate_internvl_chat_v1_5_using_vlmevalkit.md)
  - å¦‚ä½•ä½¿ç”¨ VLMEvalKit è¯„æµ‹ Mini-InternVL-Chat-2B-V1-5? (æ¨è) [\[link\]](./document/how_to_evaluate_mini_internvl_chat_2b_v1_5_using_vlmevalkit.md)
  - å¦‚ä½•ä½¿ç”¨ VLMEvalKit è¯„æµ‹ Mini-InternVL-Chat-4B-V1-5? (æ¨è) [\[link\]](./document/how_to_evaluate_mini_internvl_chat_4b_v1_5_using_vlmevalkit.md)

- éƒ¨ç½²

  - å¦‚ä½•éƒ¨ç½²æœ¬åœ°çš„ demo? [\[link\]](./document/how_to_deploy_a_local_demo.md)
  - å¦‚ä½•ç”¨ Nvidia V100 GPU è¿è¡Œ InternVL-1.5 8bit? [\[link\]](https://github.com/OpenGVLab/InternVL/issues/144) [\[ä¸­æ–‡æ•™ç¨‹\]](https://zhuanlan.zhihu.com/p/697188143)
  - å¦‚ä½•è¿›è¡Œæ‰¹é‡æ¨ç†ï¼Ÿ [\[link\]](https://github.com/OpenGVLab/InternVL/blob/main/README.md?plain=1#L617)
  - LMDeploy åŠ é€Ÿæ¨ç† [\[link\]](#inference-acceleration-by-lmdeploy) [\[ä¸­æ–‡æ•™ç¨‹\]](https://zhuanlan.zhihu.com/p/696955211)

## å’Œ SOTA å¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹æ¯”

<p align="center"><img width="500" alt="image" src="https://github.com/OpenGVLab/InternVL/assets/23737120/38e8a632-229c-4b20-b7e1-77299dfc6cee"></p>

<img width="1229" alt="image" src="https://github.com/OpenGVLab/InternVL/assets/23737120/e9065a58-86fa-47ef-be9a-eb734532e73f">

<img width="1229" alt="image" src="https://github.com/OpenGVLab/InternVL/assets/23737120/2b4f2978-36ea-4065-841d-3651c58955ed">

## ä»€ä¹ˆæ˜¯ InternVL?

InternVL å°† ViT æ‹“å±•åˆ° _**6B å‚æ•°**_ å¹¶ä¸å¤§è¯­è¨€æ¨¡å‹å¯¹é½ã€‚

## æ¨¡å‹

**å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹**

| Model                      | Date       | Download                                                                             | Note                                                                                                             |
| -------------------------- | ---------- | ------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------- |
| Mini&#8209;InternVL&#8209;Chat&#8209;4B&#8209;V1&#8209;5 | 2024.05.28 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-4B-V1-5)            | ğŸš€ğŸš€ 16% çš„æ¨¡å‹å¤§å°ï¼Œ90% çš„æ¨¡å‹æ€§èƒ½                                                                              |
| Mini-InternVL-Chat-2B-V1-5 | 2024.05.19 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5)            | ğŸš€ğŸš€ 8% çš„æ¨¡å‹å¤§å°ï¼Œ80% çš„æ¨¡å‹æ€§èƒ½                                                                               |
| InternVL-Chat-V1-5-AWQ     | 2024.05.28 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-AWQ)                | InternVL-Chat-V1-5çš„ INT4 ç‰ˆæœ¬                                                                                   |
| InternVL-Chat-V1-5-Int8    | 2024.04.28 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-Int8)               | InternVL-Chat-V1-5çš„ INT8 ç‰ˆæœ¬                                                                                   |
| InternVL-Chat-V1-5         | 2024.04.18 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5)                    | æ”¯æŒ4Kå›¾åƒï¼›è¶…å¼ºOCRæ€§èƒ½ï¼›åœ¨MMMUã€DocVQAã€ChartQAã€MathVistaç­‰å„ç§åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½æ¥è¿‘GPT-4Vå’ŒGemini Pro (ğŸ”¥æ–°) |
| InternVL-Chat-V1-2-Plus    | 2024.02.21 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus)               | æ›´å¤šçš„SFTæ•°æ®å¹¶ä¸”æ›´å¼ºå¤§                                                                                          |
| InternVL-Chat-V1-2         | 2024.02.11 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2)                    | æ‹“å±• LLM åˆ° 34B                                                                                                  |
| InternVL-Chat-V1-1         | 2024.01.24 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1)                    | æ”¯æŒä¸­æ–‡å¹¶ä¸”æœ‰å¼ºå¤§çš„OCRèƒ½åŠ›                                                                                      |
| InternVL-Chat-19B-448px    | 2024.02.03 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B-448px) | 448 åˆ†è¾¨ç‡                                                                                                       |
| InternVL-Chat-19B          | 2023.12.25 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B)       | è‹±è¯­å¤šæ¨¡æ€å¯¹è¯å¤§æ¨¡å‹                                                                                             |
| InternVL-Chat-13B          | 2023.12.25 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B)        | è‹±è¯­å¤šæ¨¡æ€å¯¹è¯å¤§æ¨¡å‹                                                                                             |

**è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹**

| Model                   | Date       | Download                                                               | Note                                                            |
| ----------------------- | ---------- | ---------------------------------------------------------------------- | --------------------------------------------------------------- |
| InternViT-300M-448px    | 2024.05.25 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternViT-300M-448px)    | è’¸é¦çš„300Må°å‹è§†è§‰åŸºç¡€æ¨¡å‹ (ğŸ”¥æ–°)                               |
| InternViT-6B-448px-V1-5 | 2024.04.20 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5) | æ”¯æŒåŠ¨æ€åˆ†è¾¨ç‡ï¼Œååˆ†å¼ºå¤§çš„OCRèƒ½åŠ› (ğŸ”¥æ–°)                        |
| InternViT-6B-448px-V1-2 | 2024.02.11 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2) | 448 åˆ†è¾¨ç‡                                                      |
| InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;0 | 2024.01.30 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-0) | 448 åˆ†è¾¨ç‡                                                      |
| InternViT-6B-224px      | 2023.12.22 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternViT-6B-224px)      | è§†è§‰åŸºç¡€æ¨¡å‹                                                    |
| InternVL-14B-224px      | 2023.12.22 | ğŸ¤— [HF link](https://huggingface.co/OpenGVLab/InternVL-14B-224px)      | è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼ŒInternViT-6B + QLLaMAï¼Œå¯ä»¥ç”¨äºåšå›¾æ–‡å¯¹çš„æ£€ç´¢ |

## InternVL å¯ä»¥åšä»€ä¹ˆ?

<details>
  <summary>è§†è§‰æ„ŸçŸ¥ (ç‚¹å‡»å±•å¼€)</summary>

- Linear-Probe å›¾åƒåˆ†ç±» [\[see details\]](./classification#-evaluation)

  ViT-22B uses the private JFT-3B dataset.

  | method              | #param | IN-1K | IN-ReaL | IN-V2 | IN-A | IN-R | IN-Sketch |
  | ------------------- | :----: | :---: | :-----: | :---: | :--: | :--: | :-------: |
  | OpenCLIP-G          |  1.8B  | 86.2  |  89.4   | 77.2  | 63.8 | 87.8 |   66.4    |
  | DINOv2-g            |  1.1B  | 86.5  |  89.6   | 78.4  | 75.9 | 78.8 |   62.5    |
  | EVA-01-CLIP-g       |  1.1B  | 86.5  |  89.3   | 77.4  | 70.5 | 87.7 |   63.1    |
  | MAWS-ViT-6.5B       |  6.5B  | 87.8  |    -    |   -   |  -   |  -   |     -     |
  | ViT-22B\*           | 21.7B  | 89.5  |  90.9   | 83.2  | 83.8 | 87.4 |     -     |
  | InternViT-6B (ours) |  5.9B  | 88.2  |  90.4   | 79.9  | 77.5 | 89.8 |   69.1    |

- è¯­ä¹‰åˆ†å‰² [\[see details\]](./segmentation#-evaluation)

  | method                | decoder | #param (train/total) | crop size | mIoU         |
  | --------------------- | :-----: | :------------------: | :-------: | ------------ |
  | OpenCLIP-G (frozen)   | Linear  |     0.3M / 1.8B      |    512    | 39.3         |
  | ViT-22B (frozen)      | Linear  |     0.9M / 21.7B     |    504    | 34.6         |
  | InternViT-6B (frozen) | Linear  |     0.5M / 5.9B      |    504    | 47.2 (+12.6) |
  | ViT-22B (frozen)      | UperNet |     0.8B / 22.5B     |    504    | 52.7         |
  | InternViT-6B (frozen) | UperNet |     0.4B / 6.3B      |    504    | 54.9 (+2.2)  |
  | ViT-22B               | UperNet |    22.5B / 22.5B     |    504    | 55.3         |
  | InternViT-6B          | UperNet |     6.3B / 6.3B      |    504    | 58.9 (+3.6)  |

- é›¶æ ·æœ¬å›¾åƒåˆ†ç±» [\[see details\]](./clip_benchmark#imagenet-variants-and-objectnet)

  | method            | IN-1K | IN-A | IN-R | IN-V2 | IN-Sketch | ObjectNet |
  | ----------------- | :---: | :--: | :--: | :---: | :-------: | :-------: |
  | OpenCLIP-G        | 80.1  | 69.3 | 92.1 | 73.6  |   68.9    |   73.0    |
  | EVA-02-CLIP-E+    | 82.0  | 82.1 | 94.5 | 75.7  |   71.6    |   79.6    |
  | ViT-22B\*         | 85.9  | 90.1 | 96.0 | 80.9  |     -     |   87.6    |
  | InternVL-C (ours) | 83.2  | 83.8 | 95.5 | 77.3  |   73.9    |   80.6    |

- å¤šè¯­è¨€é›¶æ ·æœ¬å›¾åƒåˆ†ç±» [\[see details\]](./clip_benchmark#multilingual-imagenet-1k)

  EN: English, ZH: Chinese, JP: Japanese, Ar: Arabic, IT: Italian

  | method            | IN-1K (EN) | IN-1K (ZH) | IN-1K (JP) | IN-1K (AR) | IN-1K (IT) |
  | ----------------- | :--------: | :--------: | :--------: | :--------: | :--------: |
  | Taiyi-CLIP-ViT-H  |     -      |    54.4    |     -      |     -      |     -      |
  | WuKong-ViT-L-G    |     -      |    57.5    |     -      |     -      |     -      |
  | CN-CLIP-ViT-H     |     -      |    59.6    |     -      |     -      |     -      |
  | AltCLIP-ViT-L     |    74.5    |    59.6    |     -      |     -      |     -      |
  | EVA-02-CLIP-E+    |    82.0    |     -      |     -      |     -      |    41.2    |
  | OpenCLIP-XLM-R-H  |    77.0    |    55.7    |    53.1    |    37.0    |    56.8    |
  | InternVL-C (ours) |    83.2    |    64.5    |    61.5    |    44.9    |    65.7    |

- é›¶æ ·æœ¬è§†é¢‘åˆ†ç±» \[see details\]

  | method            | #frame | K400 | K600 | K700 |
  | ----------------- | :----: | :--: | :--: | :--: |
  | OpenCLIP-G        |   1    | 65.9 | 66.1 | 59.2 |
  | EVA-02-CLIP-E+    |   1    | 69.8 | 69.3 | 63.4 |
  | InternVL-C (ours) |   1    | 71.0 | 71.3 | 65.7 |
  | ViCLIP            |   8    | 75.7 | 73.5 | 66.4 |
  | InternVL-C (ours) |   8    | 79.4 | 78.8 | 71.5 |

</details>

<details>
  <summary>è·¨æ¨¡æ€æ£€ç´¢ (ç‚¹å‡»å±•å¼€)</summary>

- è‹±è¯­é›¶æ ·æœ¬å›¾æ–‡æ£€ç´¢ [\[see details\]](./clip_benchmark#flickr30k--coco)

  <table>
    <tr  align=center>
        <td rowspan="3" align=left><b>model</b></td>
        <td colspan="6" align=center><b>Flickr30K</b></td>
        <td colspan="6" align=center><b>COCO</b></td>
        <td rowspan="3" align=center><b>avg</b></td>

  </tr>
     <tr  align=center>
        <td colspan="3" align=center><b>image-to-text</b></td>
        <td colspan="3" align=center><b>text-to-image</b></td>
         <td colspan="3" align=center><b>image-to-text</b></td>
        <td colspan="3" align=center><b>text-to-image</b></td>
     </tr>
     <tr>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
     </tr>

  <tr align=center>
        <td align=left>OpenCLIP-G</td>
        <td>92.9</td>
        <td>99.3</td>
        <td>99.8</td>
        <td>79.5</td>
        <td>95.0</td>
        <td>97.1</td>
        <td>67.3</td>
        <td>86.9</td>
        <td>92.6</td>
        <td>51.4</td>
        <td>74.9</td>
        <td>83.0</td>
        <td>85.0</td>
     </tr>
  <tr align=center>
        <td align=left>EVA-02-CLIP-E+</td>
        <td>93.9</td>
        <td>99.4</td>
        <td>99.8</td>
        <td>78.8</td>
        <td>94.2</td>
        <td>96.8</td>
        <td>68.8</td>
        <td>87.8</td>
        <td>92.8</td>
        <td>51.1</td>
        <td>75.0</td>
        <td>82.7</td>
        <td>85.1</td>
     </tr>
    <tr align=center>
        <td align=left>EVA-CLIP-8B</td>
        <td>95.6</td>
        <td>99.6</td>
        <td>99.9</td>
        <td>80.8</td>
        <td>95.5</td>
        <td>97.6</td>
        <td>70.3</td>
        <td>89.3</td>
        <td>93.9</td>
        <td>53.0</td>
        <td>76.0</td>
        <td>83.4</td>
        <td>86.2</td>
     </tr>
  <tr align=center>
        <td align=left>InternVL-C (ours)</td>
        <td>94.7</td>
        <td>99.6</td>
        <td>99.9</td>
        <td>81.7</td>
        <td>96.0</td>
        <td>98.2</td>
        <td>70.6</td>
        <td>89.0</td>
        <td>93.5</td>
        <td>54.1</td>
        <td>77.3</td>
        <td>84.6</td>
        <td>86.6</td>
     </tr>
  <tr align=center>
        <td align=left>InternVL-G (ours)</td>
        <td>95.7</td>
        <td>99.7</td>
        <td>99.9</td>
        <td>85.0</td>
        <td>97.0</td>
        <td>98.6</td>
        <td>74.9</td>
        <td>91.3</td>
        <td>95.2</td>
        <td>58.6</td>
        <td>81.3</td>
        <td>88.0</td>
        <td>88.8</td>
     </tr>

  </table>

- ä¸­æ–‡é›¶æ ·æœ¬å›¾æ–‡å¯¹æ£€ç´¢ [\[see details\]](./clip_benchmark#flickr30k-cn--coco-cn)

  <table>
    <tr  align=center>
        <td rowspan="3" align=left><b>model</b></td>
        <td colspan="6" align=center><b>Flickr30K-CN</b></td>
        <td colspan="6" align=center><b>COCO-CN</b></td>
        <td rowspan="3" align=center><b>avg</b></td>

  </tr>
     <tr  align=center>
        <td colspan="3" align=center><b>image-to-text</b></td>
        <td colspan="3" align=center><b>text-to-image</b></td>
         <td colspan="3" align=center><b>image-to-text</b></td>
        <td colspan="3" align=center><b>text-to-image</b></td>
     </tr>
     <tr>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
        <td>R@1</td>
        <td>R@5</td>
        <td>R@10</td>
     </tr>

  <tr align=center>
        <td align=left>CN-CLIP-ViT-H</td>
        <td>81.6</td>
        <td>97.5</td>
        <td>98.8</td>
        <td>71.2</td>
        <td>91.4</td>
        <td>95.5</td>
        <td>63.0</td>
        <td>86.6</td>
        <td>92.9</td>
        <td>69.2</td>
        <td>89.9</td>
        <td>96.1</td>
        <td>86.1</td>
     </tr>

  <tr align=center>
        <td align=left>OpenCLIP-XLM-R-H</td>
        <td>86.1</td>
        <td>97.5</td>
        <td>99.2</td>
        <td>71.0</td>
        <td>90.5</td>
        <td>94.9</td>
        <td>70.0</td>
        <td>91.5</td>
        <td>97.0</td>
        <td>66.1</td>
        <td>90.8</td>
        <td>96.0</td>
        <td>87.6</td>
     </tr>

  <tr align=center>
        <td align=left>InternVL-C (ours)</td>
        <td>90.3</td>
        <td>98.8</td>
        <td>99.7</td>
        <td>75.1</td>
        <td>92.9</td>
        <td>96.4</td>
        <td>68.8</td>
        <td>92.0</td>
        <td>96.7</td>
        <td>68.9</td>
        <td>91.9</td>
        <td>96.5</td>
        <td>89.0</td>
     </tr>
  <tr align=center>
        <td align=left>InternVL-G (ours)</td>
        <td>92.9</td>
        <td>99.4</td>
        <td>99.8</td>
        <td>77.7</td>
        <td>94.8</td>
        <td>97.3</td>
        <td>71.4</td>
        <td>93.9</td>
        <td>97.7</td>
        <td>73.8</td>
        <td>94.4</td>
        <td>98.1</td>
        <td>90.9</td>
     </tr>

  </table>

- å¤šè¯­è¨€é›¶æ ·æœ¬å›¾æ–‡å¯¹æ£€ç´¢ [\[see details\]](./clip_benchmark#xtd)

  | method            |  EN  |  ES  |  FR  |  ZH  |  IT  |  KO  |  RU  |  JP  | average |
  | ----------------- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :-----: |
  | AltCLIP           | 95.4 | 94.1 | 92.9 | 95.1 | 94.2 | 94.4 | 91.8 | 91.7 |  93.7   |
  | OpenCLIP-XLM-R-H  | 97.3 | 96.1 | 94.5 | 94.7 | 96.0 | 90.2 | 93.9 | 94.0 |  94.6   |
  | InternVL-C (ours) | 97.3 | 95.7 | 95.1 | 95.6 | 96.0 | 92.2 | 93.3 | 95.5 |  95.1   |
  | InternVL-G (ours) | 98.6 | 97.7 | 96.5 | 96.7 | 96.9 | 95.1 | 94.8 | 96.1 |  96.6   |

</details>

<details>
  <summary>å¤šæ¨¡æ€å¯¹è¯ (è¯·çœ‹ "å’ŒSOTAçš„å¤šæ¨¡æ€å¤§æ¨¡å‹å¯¹æ¯”")</summary>
</details>

## ä½¿ç”¨ Huggingface å¿«é€Ÿå¼€å§‹

<details>
  <summary>ä½¿ç”¨ InternViT-6B (ç‚¹å‡»å±•å¼€)</summary>

```python
import torch
from PIL import Image
from transformers import AutoModel, CLIPImageProcessor

model = AutoModel.from_pretrained(
    'OpenGVLab/InternViT-6B-224px',
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).cuda().eval()

image = Image.open('./examples/image1.jpg').convert('RGB')

image_processor = CLIPImageProcessor.from_pretrained('OpenGVLab/InternViT-6B-224px')

pixel_values = image_processor(images=image, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()

outputs = model(pixel_values)
```

</details>

<details>
  <summary>ä½¿ç”¨ InternVL-C(ontrastive) å’Œ InternVL-G(enerative) (ç‚¹å‡»å±•å¼€)</summary>

```python
import torch
from PIL import Image
from transformers import AutoModel, CLIPImageProcessor
from transformers import AutoTokenizer


model = AutoModel.from_pretrained(
    'OpenGVLab/InternVL-14B-224px',
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).cuda().eval()

image_processor = CLIPImageProcessor.from_pretrained('OpenGVLab/InternVL-14B-224px')

tokenizer = AutoTokenizer.from_pretrained(
    'OpenGVLab/InternVL-14B-224px', use_fast=False, add_eos_token=True)
tokenizer.pad_token_id = 0  # set pad_token_id to 0

images = [
    Image.open('./examples/image1.jpg').convert('RGB'),
    Image.open('./examples/image2.jpg').convert('RGB'),
    Image.open('./examples/image3.jpg').convert('RGB')
]
prefix = 'summarize:'
texts = [
    prefix + 'a photo of a red panda',  # English
    prefix + 'ä¸€å¼ ç†ŠçŒ«çš„ç…§ç‰‡',  # Chinese
    prefix + 'äºŒåŒ¹ã®çŒ«ã®å†™çœŸ'  # Japanese
]

pixel_values = image_processor(images=images, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()
input_ids = tokenizer(texts, return_tensors='pt', max_length=80,
                      truncation=True, padding='max_length').input_ids.cuda()

# InternVL-C
logits_per_image, logits_per_text = model(
    image=pixel_values, text=input_ids, mode='InternVL-C')
probs = logits_per_image.softmax(dim=-1)
# tensor([[9.9609e-01, 5.2185e-03, 6.0070e-08],
#         [2.2949e-02, 9.7656e-01, 5.9903e-06],
#         [3.2932e-06, 7.4863e-05, 1.0000e+00]], device='cuda:0',
#        dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)

# InternVL-G
logits_per_image, logits_per_text = model(
    image=pixel_values, text=input_ids, mode='InternVL-G')
probs = logits_per_image.softmax(dim=-1)
# tensor([[9.9609e-01, 3.1738e-03, 3.6322e-08],
#         [8.6060e-03, 9.9219e-01, 2.8759e-06],
#         [1.7583e-06, 3.1233e-05, 1.0000e+00]], device='cuda:0',
#        dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)

# please set add_eos_token to False for generation
tokenizer.add_eos_token = False
image = Image.open('./examples/image1.jpg').convert('RGB')
pixel_values = image_processor(images=image, return_tensors='pt').pixel_values
pixel_values = pixel_values.to(torch.bfloat16).cuda()

tokenized = tokenizer("English caption:", return_tensors='pt')
pred = model.generate(
    pixel_values=pixel_values,
    input_ids=tokenized.input_ids.cuda(),
    attention_mask=tokenized.attention_mask.cuda(),
    num_beams=5,
    min_new_tokens=8,
)
caption = tokenizer.decode(pred[0].cpu(), skip_special_tokens=True).strip()
# English caption: a red panda sitting on top of a wooden platform
```

</details>

<details>
  <summary>ä½¿ç”¨ InternVL-Chat (ç‚¹å‡»å±•å¼€)</summary>

```python
from transformers import AutoTokenizer, AutoModel
import torch
import torchvision.transforms as T
from PIL import Image

from torchvision.transforms.functional import InterpolationMode


IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD = (0.229, 0.224, 0.225)


def build_transform(input_size):
    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD
    transform = T.Compose([
        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),
        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),
        T.ToTensor(),
        T.Normalize(mean=MEAN, std=STD)
    ])
    return transform


def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):
    best_ratio_diff = float('inf')
    best_ratio = (1, 1)
    area = width * height
    for ratio in target_ratios:
        target_aspect_ratio = ratio[0] / ratio[1]
        ratio_diff = abs(aspect_ratio - target_aspect_ratio)
        if ratio_diff < best_ratio_diff:
            best_ratio_diff = ratio_diff
            best_ratio = ratio
        elif ratio_diff == best_ratio_diff:
            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:
                best_ratio = ratio
    return best_ratio


def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size
    aspect_ratio = orig_width / orig_height

    # calculate the existing image aspect ratio
    target_ratios = set(
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])

    # find the closest aspect ratio to the target
    target_aspect_ratio = find_closest_aspect_ratio(
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)

    # calculate the target width and height
    target_width = image_size * target_aspect_ratio[0]
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]

    # resize the image
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        # split the image
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images


def load_image(image_file, input_size=448, max_num=6):
    image = Image.open(image_file).convert('RGB')
    transform = build_transform(input_size=input_size)
    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)
    pixel_values = [transform(image) for image in images]
    pixel_values = torch.stack(pixel_values)
    return pixel_values


path = "OpenGVLab/InternVL-Chat-V1-5"
# If you have an 80G A100 GPU, you can put the entire model on a single GPU.
model = AutoModel.from_pretrained(
    path,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True).eval().cuda()
# Otherwise, you need to set device_map='auto' to use multiple GPUs for inference.
# model = AutoModel.from_pretrained(
#     path,
#     torch_dtype=torch.bfloat16,
#     low_cpu_mem_usage=True,
#     trust_remote_code=True,
#     device_map='auto').eval()

tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
# set the max number of tiles in `max_num`
pixel_values = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()

generation_config = dict(
    num_beams=1,
    max_new_tokens=512,
    do_sample=False,
)

# single-round single-image conversation
question = "è¯·è¯¦ç»†æè¿°å›¾ç‰‡" # Please describe the picture in detail
response = model.chat(tokenizer, pixel_values, question, generation_config)
print(question, response)

# multi-round single-image conversation
question = "è¯·è¯¦ç»†æè¿°å›¾ç‰‡" # Please describe the picture in detail
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)
print(question, response)

question = "è¯·æ ¹æ®å›¾ç‰‡å†™ä¸€é¦–è¯—" # Please write a poem according to the picture
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)
print(question, response)

# multi-round multi-image conversation
pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()
pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()
pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)

question = "è¯¦ç»†æè¿°è¿™ä¸¤å¼ å›¾ç‰‡" # Describe the two pictures in detail
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)
print(question, response)

question = "è¿™ä¸¤å¼ å›¾ç‰‡çš„ç›¸åŒç‚¹å’ŒåŒºåˆ«åˆ†åˆ«æ˜¯ä»€ä¹ˆ" # What are the similarities and differences between these two pictures
response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)
print(question, response)

# batch inference (single image per sample)
pixel_values1 = load_image('./examples/image1.jpg', max_num=6).to(torch.bfloat16).cuda()
pixel_values2 = load_image('./examples/image2.jpg', max_num=6).to(torch.bfloat16).cuda()
image_counts = [pixel_values1.size(0), pixel_values2.size(0)]
pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)

questions = ["Describe the image in detail."] * len(image_counts)
responses = model.batch_chat(tokenizer, pixel_values,
                             image_counts=image_counts,
                             questions=questions,
                             generation_config=generation_config)
for question, response in zip(questions, responses):
    print(question)
    print(response)
```

</details>

## é€šè¿‡ LMDeploy åŠ é€Ÿæ¨ç†

å¦‚æœéœ€è¦ä¼˜åŒ–InternVL-Chatæ¨¡å‹çš„æ¨ç†ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ [LMDeploy](https://github.com/InternLM/lmdeploy)ã€‚

åœ¨æ¥ä¸‹æ¥çš„å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥ [InternVL-Chat-V1-5](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5) æ¨¡å‹ä¸ºä¾‹ä»‹ç» LMDeploy çš„ä½¿ç”¨

é¦–å…ˆï¼Œè¯·æŒ‰ç…§ä¸‹é¢çš„æ­¥éª¤è®¾ç½®æ¨ç†ç¯å¢ƒ:

```shell
conda create -n internvl python=3.10 -y
conda activate internvl

pip install timm torchvision==0.17.2
pip install lmdeploy
```

LMDeploy çš„ pypi åŒ…é»˜è®¤ä¾èµ– CUDA 12.xã€‚å¯¹äº CUDA 11.x ç¯å¢ƒï¼Œè¯·å‚è€ƒ  [installation guide](https://lmdeploy.readthedocs.io/en/latest/get_started.html#installation).

### ç¦»çº¿æ¨ç†è¿‡ç¨‹

```python
from lmdeploy import pipeline
from lmdeploy.vl import load_image
pipe = pipeline('OpenGVLab/InternVL-Chat-V1-5')
image = load_image('examples/image2.jpg')
response = pipe(('describe this image', image))
print(response)
```

æœ‰å…³ä½¿ç”¨VLMæµç¨‹çš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å›¾åƒæ¨ç†æˆ–å¤šè½®å¯¹è¯ï¼Œè¯·æŸ¥çœ‹æŒ‡å— [guide](https://lmdeploy.readthedocs.io/en/latest/inference/vl_pipeline.html) ã€‚

### åœ¨çº¿æ¨ç†æœåŠ¡

LMDeployæ”¯æŒå°†VLMæ¨¡å‹ä¸€é”®æ‰“åŒ…æˆOpenAIæœåŠ¡ï¼Œå®ç°ä¸OpenAI APIçš„æ— ç¼é›†æˆã€‚

è¯¥æœåŠ¡å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ï¼š

```shell
lmdeploy serve api_server OpenGVLab/InternVL-Chat-V1-5
```

`api_server`çš„å‚æ•°å¯ä»¥é€šè¿‡å‘½ä»¤`lmdeploy serve api_server -h`æŸ¥çœ‹ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨`--tp`è®¾ç½®å¼ é‡å¹¶è¡Œåº¦ï¼Œä½¿ç”¨`--session-len`æŒ‡å®šä¸Šä¸‹æ–‡çª—å£çš„æœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨`--cache-max-entry-count`è°ƒæ•´ç”¨äºk/vç¼“å­˜çš„GPUå†…å­˜æ¯”ä¾‹ç­‰ã€‚

æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½¿ç”¨Dockerå¯åŠ¨æœåŠ¡ã€RESTful APIä¿¡æ¯ä»¥åŠOpenAIé›†æˆæ–¹æ³•ï¼Œè¯·æŸ¥çœ‹æŒ‡å¯¼ [guide](https://lmdeploy.readthedocs.io/en/latest/serving/api_server_vl.html)ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ª[MIT license](LICENSE)è®¸å¯è¯å‘å¸ƒã€‚é¡¹ç›®ä¸­çš„éƒ¨åˆ†ä»£ç å’Œæ¨¡å‹æ¥è‡ªå…¶ä»–æ¥æºï¼Œå¹¶å—å…¶å„è‡ªè®¸å¯è¯çš„çº¦æŸã€‚

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­å‘ç°æœ¬é¡¹ç›®æœ‰ç”¨ï¼Œè¯·è€ƒè™‘å¼•ç”¨ï¼š

```BibTeX
@article{chen2023internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}

@article{chen2024far,
  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}
```

## æ„Ÿè°¢

InternVL çš„ä»£ç æ„å»ºå‚è€ƒäº†ä»¥ä¸‹é¡¹ç›®: [OpenAI CLIP](https://github.com/openai/CLIP)ã€[Open CLIP](https://github.com/mlfoundations/open_clip)ã€[CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark)ã€[EVA](https://github.com/baaivision/EVA/tree/master)ã€[InternImage](https://github.com/OpenGVLab/InternImage)ã€[ViT-Adapter](https://github.com/czczup/ViT-Adapter)ã€[MMSegmentation](https://github.com/open-mmlab/mmsegmentation)ã€[Transformers](https://github.com/huggingface/transformers)ã€[DINOv2](https://github.com/facebookresearch/dinov2)ã€[BLIP-2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)ã€[Qwen-VL](https://github.com/QwenLM/Qwen-VL/tree/master/eval_mm)å’Œ [LLaVA-1.5](https://github.com/haotian-liu/LLaVA)ã€‚æ„Ÿè°¢ä»–ä»¬çš„å·¥ä½œã€‚

______________________________________________________________________

å¦‚ä½•ä½ æƒ³åŠ å…¥æˆ‘ä»¬çš„é¡¹ç›®ç¾¤ï¼Œè¯·æ‰«æä¸‹æ–¹äºŒç»´ç æ·»åŠ æˆ‘ä»¬çš„å°åŠ©æ‰‹ã€‚

<p align="center"><img width="300" alt="image" src="https://github.com/OpenGVLab/DragGAN/assets/26198430/e3f0807f-956a-474e-8fd2-1f7c22d73997"></p>
