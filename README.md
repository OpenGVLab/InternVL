# InternVL

The official implementation of

[InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](<>).

\[[Paper](<>)\]  \[[Demo](<>)\]

## Hightlights

- Scaling up ViT to 6B parameters
- A good alternative to the ViT-22B
- State-of-the-art performance on 32 generic visual-linguistic benchmarks

<img width="1204" alt="image" src="https://github.com/OpenGVLab/InternVL/assets/23737120/bb8929c8-b653-45ef-a0fc-628a798a1a62">

## Schedule

- [ ] Release InternVL-Chat (our custom codebase)
- [ ] Release InternVL-Chat (LLaVA codebase)
- [ ] Release InternVL
- [ ] Release InternViT-6B

## Overview

<img width="1204" alt="image" src="https://github.com/OpenGVLab/InternVL/assets/23737120/d081bdcf-525a-4f84-98d8-30a0c353d6e9">

## Performance

<img width="1204" alt="image" src="https://github.com/OpenGVLab/InternVL/assets/23737120/c9f93b54-fdba-4a69-9341-e905376f7b9c">

## Model Weights

| model name   | type        | download |
| ------------ | ----------- | -------- |
| InternViT-6B | pytorch     |          |
| InternViT-6B | huggingface |          |
| InternVL     | pytorch     |          |
| InternVL     |             |          |

## License

This project is released under the [MIT license](LICENSE).

## Citation

If you find this project useful in your research, please consider cite:

```BibTeX
TODO
```

## Acknowledgement

InternVL is built with [OpenAI CLIP](https://github.com/openai/CLIP), [Open CLIP](https://github.com/mlfoundations/open_clip), [CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark), [EVA](https://github.com/baaivision/EVA/tree/master), [InternImage](https://github.com/OpenGVLab/InternImage), [ViT-Adapter](https://github.com/czczup/ViT-Adapter), [MMSegmentation](https://github.com/open-mmlab/mmsegmentation), [Transformers](https://github.com/huggingface/transformers), [DINOv2](https://github.com/facebookresearch/dinov2), [BLIP-2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2), [Qwen-VL](https://github.com/QwenLM/Qwen-VL/tree/master/eval_mm), and [LLaVA-1.5](https://github.com/haotian-liu/LLaVA). Thanks for their awesome work!
